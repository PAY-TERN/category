#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ì¹´ë“œ ë¶„ë¥˜ê¸° FastAPI ë°±ì—”ë“œ - í•˜ë‚˜ì˜ í†µí•© ëª¨ë¸ ì§€ì†ì  ì—…ë°ì´íŠ¸
"""

from fastapi import FastAPI, UploadFile, File, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import FileResponse, JSONResponse, Response
from pydantic import BaseModel
from typing import List, Dict, Optional, Any
import pandas as pd
import numpy as np
import json
import uuid
import subprocess
from pathlib import Path
import shutil
from datetime import datetime, timedelta
import logging
import re
from io import BytesIO

# í†µí•© ëª¨ë¸ ë¶„ë¥˜ê¸° import
from enhanced_classifier import PersistentIncrementalCategorizer

# FastAPI ì•± ì´ˆê¸°í™”
app = FastAPI(
    title="ì¹´ë“œ ë¶„ë¥˜ê¸° API (í†µí•© ëª¨ë¸)",
    description="í•˜ë‚˜ì˜ ëª¨ë¸ì„ ì§€ì†ì ìœ¼ë¡œ ì—…ë°ì´íŠ¸í•˜ëŠ” ì ì§„ì  í•™ìŠµ ì‹œìŠ¤í…œ",
    version="3.1.0"
)

# CORS ì„¤ì •
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# ì „ì—­ ë³€ìˆ˜
classifier_instances = {}  # ì„¸ì…˜ë³„ ë¶„ë¥˜ê¸° ì¸ìŠ¤í„´ìŠ¤ ê´€ë¦¬
sessions = {}  # ì„¸ì…˜ ë°ì´í„° ì €ì¥
temp_dir = Path("temp_files")
temp_dir.mkdir(exist_ok=True)

# í•˜ë‚˜ì˜ ê¸€ë¡œë²Œ ë¶„ë¥˜ê¸° ì¸ìŠ¤í„´ìŠ¤ (ëª¨ë“  ì„¸ì…˜ì´ ê³µìœ )
global_classifier = None

# NaN ì•ˆì „ ì²˜ë¦¬ í•¨ìˆ˜ë“¤
def clean_for_json(obj):
    """ëª¨ë“  NaN ê°’ì„ ì•ˆì „í•˜ê²Œ ì²˜ë¦¬í•˜ì—¬ JSON ì§ë ¬í™” ê°€ëŠ¥í•˜ê²Œ ë§Œë“¦"""
    if obj is None:
        return None
    elif isinstance(obj, dict):
        return {k: clean_for_json(v) for k, v in obj.items()}
    elif isinstance(obj, (list, tuple)):
        return [clean_for_json(item) for item in obj]
    elif isinstance(obj, pd.DataFrame):
        df_clean = obj.fillna('')
        return clean_for_json(df_clean.to_dict('records'))
    elif isinstance(obj, pd.Series):
        series_clean = obj.fillna('')
        return clean_for_json(series_clean.to_dict())
    elif isinstance(obj, np.ndarray):
        return clean_for_json(obj.tolist())
    elif pd.isna(obj):
        return None
    elif isinstance(obj, (np.floating, float)):
        if np.isnan(obj) or np.isinf(obj):
            return None
        return float(obj)
    elif isinstance(obj, (np.integer, int)):
        if pd.isna(obj):
            return None
        return int(obj)
    elif isinstance(obj, np.bool_):
        return bool(obj)
    elif isinstance(obj, (pd.Timestamp, np.datetime64)):
        try:
            return str(obj)
        except:
            return None
    else:
        return obj

class SafeJSONResponse(JSONResponse):
    """NaN ê°’ì„ ìë™ìœ¼ë¡œ ì²˜ë¦¬í•˜ëŠ” ì•ˆì „í•œ JSON ì‘ë‹µ"""
    
    def render(self, content: Any) -> bytes:
        try:
            cleaned_content = clean_for_json(content)
            json_str = json.dumps(
                cleaned_content,
                ensure_ascii=False,
                allow_nan=False,
                indent=None,
                separators=(",", ":"),
            )
            return json_str.encode("utf-8")
        except (TypeError, ValueError) as e:
            logger.error(f"JSON ì§ë ¬í™” ì˜¤ë¥˜: {e}")
            error_content = {
                "error": "JSON ì§ë ¬í™” ì‹¤íŒ¨",
                "message": str(e),
                "original_type": str(type(content).__name__)
            }
            json_str = json.dumps(error_content, ensure_ascii=False)
            return json_str.encode("utf-8")

def get_global_classifier():
    """ê¸€ë¡œë²Œ ë¶„ë¥˜ê¸° ì¸ìŠ¤í„´ìŠ¤ ê°€ì ¸ì˜¤ê¸°"""
    global global_classifier
    if global_classifier is None:
        global_classifier = PersistentIncrementalCategorizer()
        logger.info("ğŸ¯ í†µí•© ëª¨ë¸ ë¶„ë¥˜ê¸° ì´ˆê¸°í™” ì™„ë£Œ")
    return global_classifier

# Ollama ì—°ê²° í™•ì¸ í•¨ìˆ˜ë“¤
def check_ollama_connection():
    """Ollama ì„œë¹„ìŠ¤ ì—°ê²° í™•ì¸"""
    try:
        result = subprocess.run(
            ['ollama', 'list'],
            capture_output=True,
            timeout=5
        )
        return result.returncode == 0
    except Exception as e:
        logger.warning(f"Ollama ì—°ê²° í™•ì¸ ì‹¤íŒ¨: {e}")
        return False

def get_available_ollama_models():
    """ì‚¬ìš© ê°€ëŠ¥í•œ Ollama ëª¨ë¸ ëª©ë¡ ì¡°íšŒ"""
    try:
        result = subprocess.run(
            ['ollama', 'list'],
            capture_output=True,
            text=True,
            timeout=10
        )
        if result.returncode == 0:
            lines = result.stdout.strip().split('\n')
            models = []
            for line in lines[1:]:  # í—¤ë” ì œì™¸
                if line.strip():
                    model_name = line.split()[0]
                    models.append(model_name)
            return models
        return []
    except Exception as e:
        logger.error(f"Ollama ëª¨ë¸ ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨: {e}")
        return []

# ë‚ ì§œ ë³€í™˜ í•¨ìˆ˜ë“¤
def convert_excel_date(value):
    """ì—‘ì…€ ì‹œë¦¬ì–¼ ë‚ ì§œë¥¼ ì‹¤ì œ ë‚ ì§œë¡œ ë³€í™˜"""
    if pd.isna(value):
        return None
    
    if isinstance(value, (int, float)):
        try:
            if value >= 60:
                excel_epoch = datetime(1899, 12, 30)
            else:
                excel_epoch = datetime(1899, 12, 31)
            
            converted_date = excel_epoch + timedelta(days=int(value))
            return converted_date.strftime('%Y-%m-%d')
        except Exception as e:
            logger.warning(f"ë‚ ì§œ ë³€í™˜ ì‹¤íŒ¨: {value} -> {e}")
            return str(value)
    
    elif isinstance(value, datetime):
        return value.strftime('%Y-%m-%d')
    
    elif isinstance(value, str):
        if re.match(r'\d{4}-\d{2}-\d{2}', value):
            return value
        
        try:
            parsed_date = pd.to_datetime(value)
            return parsed_date.strftime('%Y-%m-%d')
        except:
            return value
    
    return str(value)

def is_likely_excel_date(series, threshold=0.3):
    """ì‹œë¦¬ì¦ˆê°€ ì—‘ì…€ ë‚ ì§œ ì»¬ëŸ¼ì¸ì§€ íŒë‹¨"""
    if series.dtype not in ['int64', 'float64']:
        return False
        
    numeric_values = series.dropna()
    if len(numeric_values) == 0:
        return False
    
    date_range_values = numeric_values[(numeric_values >= 1) & (numeric_values <= 73050)]
    recent_date_values = numeric_values[(numeric_values >= 40000) & (numeric_values <= 50000)]
    
    recent_ratio = len(recent_date_values) / len(numeric_values)
    total_ratio = len(date_range_values) / len(numeric_values)
    
    return recent_ratio >= threshold or total_ratio >= 0.5

def preprocess_excel_data(df: pd.DataFrame) -> tuple:
    """ì—‘ì…€ ë°ì´í„° ì „ì²˜ë¦¬ - NaN ì•ˆì „ ì²˜ë¦¬"""
    df_processed = df.copy()
    df_processed = df_processed.fillna('')  # ë¨¼ì € ëª¨ë“  NaN ì²˜ë¦¬
    
    conversion_info = []
    
    logger.info("ì—‘ì…€ ë°ì´í„° ì „ì²˜ë¦¬ ì‹œì‘")
    
    date_keywords = ['ì¼ì', 'ë‚ ì§œ', 'date', 'day', 'ì‚¬ìš©ì¼', 'ìŠ¹ì¸ì¼', 'ê±°ë˜ì¼', 'ì´ìš©ì¼']
    
    for col in df_processed.columns:
        original_values = df_processed[col].copy()
        should_convert = False
        conversion_method = None
        
        col_lower = col.lower()
        if any(keyword in col_lower for keyword in date_keywords):
            logger.info(f"ë‚ ì§œ í‚¤ì›Œë“œ ê°ì§€: {col}")
            should_convert = True
            conversion_method = "keyword_based"
        
        elif is_likely_excel_date(df_processed[col]):
            logger.info(f"ì—‘ì…€ ë‚ ì§œë¡œ ì¶”ì •: {col}")
            should_convert = True
            conversion_method = "numeric_analysis"
        
        if should_convert:
            try:
                converted_values = df_processed[col].apply(convert_excel_date)
                converted_values = converted_values.fillna('')  # NaN ì²˜ë¦¬
                
                valid_conversions = 0
                total_non_null = 0
                
                for orig, conv in zip(original_values.dropna(), converted_values.dropna()):
                    total_non_null += 1
                    if isinstance(conv, str) and re.match(r'\d{4}-\d{2}-\d{2}', conv):
                        valid_conversions += 1
                
                if total_non_null > 0 and (valid_conversions / total_non_null) >= 0.5:
                    df_processed[col] = converted_values
                    
                    sample_original = str(original_values.dropna().iloc[0]) if len(original_values.dropna()) > 0 else ''
                    sample_converted = str(converted_values.dropna().iloc[0]) if len(converted_values.dropna()) > 0 else ''
                    
                    conversion_info.append({
                        'original': col,
                        'converted': col,
                        'method': conversion_method,
                        'sample_original': sample_original,
                        'sample_converted': sample_converted,
                        'success_rate': valid_conversions / total_non_null
                    })
                    
                    logger.info(f"ë‚ ì§œ ë³€í™˜ ì™„ë£Œ: {col} (ì„±ê³µë¥ : {valid_conversions}/{total_non_null})")
                else:
                    logger.warning(f"ë‚ ì§œ ë³€í™˜ ì‹¤íŒ¨: {col} (ì„±ê³µë¥  ë‚®ìŒ: {valid_conversions}/{total_non_null})")
                    
            except Exception as e:
                logger.error(f"ë‚ ì§œ ë³€í™˜ ì¤‘ ì˜¤ë¥˜: {col} -> {e}")
    
    # ìµœì¢… NaN ì²˜ë¦¬
    df_processed = df_processed.fillna('')
    
    logger.info(f"ë‚ ì§œ ì „ì²˜ë¦¬ ì™„ë£Œ. ë³€í™˜ëœ ì»¬ëŸ¼: {len(conversion_info)}ê°œ")
    return df_processed, conversion_info

# Pydantic ëª¨ë¸ë“¤
class ClassificationRequest(BaseModel):
    session_id: str
    merchant_column: str = "ê°€ë§¹ì ëª…"
    date_column: str = "ì´ìš©ì¼ì"

class BatchProcessRequest(BaseModel):
    session_id: str
    batch_number: int
    corrections: Dict[str, str]

class PredictionRequest(BaseModel):
    merchant_names: List[str]
    session_id: str

class BatchInfo(BaseModel):
    batch_number: int
    month: Optional[str]
    data: List[Dict[str, Any]]
    classifications: Dict[str, str]
    confidence_scores: Dict[str, float]

class PerformanceInfo(BaseModel):
    batch: int
    train_accuracy: float
    cv_mean: float
    cv_std: float
    training_samples: int
    categories: int
    confidence_threshold: float

# API ì—”ë“œí¬ì¸íŠ¸ë“¤
@app.get("/")
async def root():
    """ë£¨íŠ¸ ì—”ë“œí¬ì¸íŠ¸ - NaN ì•ˆì „ ì²˜ë¦¬"""
    try:
        ollama_status = check_ollama_connection()
        available_models = get_available_ollama_models() if ollama_status else []
        
        # í†µí•© ëª¨ë¸ ìƒíƒœ í™•ì¸
        classifier = get_global_classifier()
        model_status = classifier.get_model_status()
        
        response_data = {
            "message": "ì¹´ë“œ ë¶„ë¥˜ê¸° API (í†µí•© ëª¨ë¸)",
            "version": "3.1.0",
            "status": "running",
            "ollama_connected": ollama_status,
            "available_ollama_models": available_models,
            "unified_model_status": model_status,
            "features": [
                "í•˜ë‚˜ì˜ í†µí•© ëª¨ë¸ ì§€ì†ì  ì—…ë°ì´íŠ¸",
                "ì›”ë³„ ë°°ì¹˜ ìë™ ë¶„í• ",
                "ê¸°ì¡´ í•™ìŠµ ë°ì´í„° ìë™ ë¡œë“œ",
                "ì¤‘ë‹¨ í›„ ì¬ì‹œì‘ ê°€ëŠ¥",
                "ML ë¶„ë¥˜ ë””ë²„ê¹… ê°•í™”"
            ]
        }
        
        return SafeJSONResponse(content=response_data)
        
    except Exception as e:
        logger.error(f"ë£¨íŠ¸ ì—”ë“œí¬ì¸íŠ¸ ì˜¤ë¥˜: {e}")
        error_response = {
            "error": "ì„œë²„ ì˜¤ë¥˜",
            "message": str(e)
        }
        return SafeJSONResponse(content=error_response, status_code=500)

@app.get("/model-status")
async def get_unified_model_status():
    """í†µí•© ëª¨ë¸ ìƒíƒœ ì¡°íšŒ - NaN ì•ˆì „ ì²˜ë¦¬"""
    try:
        classifier = get_global_classifier()
        status = classifier.get_model_status()
        
        category_distribution = {}
        if len(classifier.all_training_data) > 0:
            try:
                category_counts = classifier.all_training_data['ì¹´í…Œê³ ë¦¬'].value_counts()
                category_distribution = category_counts.to_dict()
            except Exception as e:
                logger.warning(f"ì¹´í…Œê³ ë¦¬ ë¶„í¬ ê³„ì‚° ì‹¤íŒ¨: {e}")
        
        response_data = {
            "model_info": status,
            "performance_history": classifier.performance_history[-10:] if classifier.performance_history else [],
            "category_distribution": category_distribution
        }
        
        return SafeJSONResponse(content=response_data)
        
    except Exception as e:
        logger.error(f"ëª¨ë¸ ìƒíƒœ ì¡°íšŒ ì˜¤ë¥˜: {e}")
        error_response = {
            "error": "ëª¨ë¸ ìƒíƒœ ì¡°íšŒ ì‹¤íŒ¨",
            "message": str(e)
        }
        return SafeJSONResponse(content=error_response, status_code=500)

@app.get("/ollama-status")
async def get_ollama_status():
    """Ollama ì—°ê²° ìƒíƒœ í™•ì¸"""
    ollama_connected = check_ollama_connection()
    available_models = get_available_ollama_models() if ollama_connected else []
    
    response_data = {
        "connected": ollama_connected,
        "models": available_models,
        "recommended_models": ["llama3", "llama3.1", "gemma2", "mistral"],
        "install_command": "ollama pull llama3" if not available_models else None
    }
    
    return SafeJSONResponse(content=response_data)

@app.post("/upload")
async def upload_file(file: UploadFile = File(...)):
    """ì—‘ì…€ íŒŒì¼ ì—…ë¡œë“œ ë° ì„¸ì…˜ ìƒì„± - NaN ì•ˆì „ ì²˜ë¦¬"""
    try:
        if not file.filename.endswith(('.xlsx', '.xls')):
            raise HTTPException(status_code=400, detail="ì—‘ì…€ íŒŒì¼ë§Œ ì—…ë¡œë“œ ê°€ëŠ¥í•©ë‹ˆë‹¤.")
        
        session_id = str(uuid.uuid4())
        
        # íŒŒì¼ ì½ê¸°
        content = await file.read()
        df_original = pd.read_excel(BytesIO(content))
        
        # NaN ê°’ ì²˜ë¦¬
        df_original = df_original.fillna('')
        
        # ë‚ ì§œ ì „ì²˜ë¦¬
        df_processed, conversion_info = preprocess_excel_data(df_original)
        
        # ì„ì‹œ íŒŒì¼ ì €ì¥
        temp_file_path = temp_dir / f"{session_id}_{file.filename}"
        df_processed.to_excel(temp_file_path, index=False)
        
        # ë¯¸ë¦¬ë³´ê¸° ë°ì´í„° NaN ì²˜ë¦¬
        preview_data = clean_for_json(df_processed.head(5).to_dict('records'))
        
        # ì„¸ì…˜ ë°ì´í„°ì— ì €ì¥
        sessions[session_id] = {
            'filename': file.filename,
            'original_data': df_processed,
            'upload_time': datetime.now().isoformat(),
            'total_rows': len(df_processed),
            'columns': df_processed.columns.tolist(),
            'preview': preview_data,
            'classifications': {},
            'status': 'uploaded',
            'merchant_column': 'ê°€ë§¹ì ëª…',
            'total_batches': 0
        }
        
        logger.info(f"íŒŒì¼ ì—…ë¡œë“œ ì™„ë£Œ: {file.filename}, ì„¸ì…˜: {session_id}")
        
        response_data = {
            "session_id": session_id,
            "filename": file.filename,
            "total_rows": len(df_processed),
            "columns": df_processed.columns.tolist(),
            "preview": preview_data,
            "date_conversions": conversion_info,
            "status": "uploaded"
        }
        
        return SafeJSONResponse(content=response_data)
        
    except Exception as e:
        logger.error(f"íŒŒì¼ ì—…ë¡œë“œ ì˜¤ë¥˜: {e}")
        error_response = {
            "error": "íŒŒì¼ ì²˜ë¦¬ ì‹¤íŒ¨",
            "message": str(e)
        }
        return SafeJSONResponse(content=error_response, status_code=400)

@app.post("/start-classification")
async def start_classification(request: ClassificationRequest):
    """ë¶„ë¥˜ í”„ë¡œì„¸ìŠ¤ ì‹œì‘ - NaN ì•ˆì „ ì²˜ë¦¬"""
    try:
        session_id = request.session_id
        
        if session_id not in sessions:
            raise HTTPException(status_code=404, detail="ì„¸ì…˜ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        
        session_data = sessions[session_id]
        df = session_data['original_data'].copy()
        df = df.fillna('')  # ì¶”ê°€ NaN ì²˜ë¦¬
        
        session_data['merchant_column'] = request.merchant_column
        
        # Ollama ì—°ê²° í™•ì¸
        ollama_connected = check_ollama_connection()
        if not ollama_connected:
            logger.warning("Ollamaê°€ ì—°ê²°ë˜ì§€ ì•ŠìŒ. í‚¤ì›Œë“œ ê¸°ë°˜ ë¶„ë¥˜ë§Œ ì‚¬ìš©ë©ë‹ˆë‹¤.")
        
        # ê¸€ë¡œë²Œ í†µí•© ëª¨ë¸ ì‚¬ìš©
        classifier = get_global_classifier()
        classifier.merchant_column = request.merchant_column
        
        # ë‚ ì§œ ì»¬ëŸ¼ ìë™ ê°ì§€
        date_column_candidates = ['ì´ìš©ì¼ì', 'ìŠ¹ì¸ì¼', 'ê±°ë˜ì¼', 'ì¼ì', 'ë‚ ì§œ']
        detected_date_col = None
        
        for col in df.columns:
            if any(keyword in col for keyword in date_column_candidates):
                detected_date_col = col
                break

        if not detected_date_col:
            raise HTTPException(status_code=400, detail="ë‚ ì§œ ì»¬ëŸ¼ì„ ìë™ìœ¼ë¡œ ì¸ì‹í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

        classifier.date_column = detected_date_col
        
        # ì›”ë³„ ë°°ì¹˜ ìƒì„±
        monthly_batches = classifier.create_monthly_batches(df)
        
        if not monthly_batches:
            raise HTTPException(status_code=400, detail="ì›”ë³„ ë°°ì¹˜ë¥¼ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        
        session_data['total_batches'] = len(monthly_batches)
        
        model_status = classifier.get_model_status()
        
        classifier_instances[session_id] = {
            "classifier": classifier,
            "batches": monthly_batches,
            "current_batch": 0,
            "total_batches": len(monthly_batches),
            "status": "ready",
            "created_at": datetime.now(),
            "ollama_available": ollama_connected
        }
        
        # ë°°ì¹˜ ì •ë³´ ì•ˆì „ ì²˜ë¦¬
        batch_info = []
        for i, batch in enumerate(monthly_batches):
            try:
                month = batch['ë…„ì›”'].iloc[0] if 'ë…„ì›”' in batch.columns else f"ë°°ì¹˜{i+1}"
                count = len(batch)
                batch_info.append({"month": str(month), "count": int(count)})
            except Exception as e:
                logger.warning(f"ë°°ì¹˜ {i+1} ì •ë³´ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
                batch_info.append({"month": f"ë°°ì¹˜{i+1}", "count": 0})
        
        response_data = {
            "session_id": session_id,
            "total_batches": len(monthly_batches),
            "total_samples": len(df),
            "status": "ready",
            "ollama_available": ollama_connected,
            "model_status": model_status,
            "batch_info": batch_info,
            "message": f"ì›”ë³„ ë¶„ë¥˜ í”„ë¡œì„¸ìŠ¤ê°€ ì‹œì‘ë˜ì—ˆìŠµë‹ˆë‹¤. (í†µí•© ëª¨ë¸ ì‚¬ìš©)"
        }
        
        return SafeJSONResponse(content=response_data)
        
    except Exception as e:
        logger.error(f"ë¶„ë¥˜ ì‹œì‘ ì˜¤ë¥˜: {e}")
        error_response = {
            "error": "ë¶„ë¥˜ ì‹œì‘ ì‹¤íŒ¨",
            "message": str(e)
        }
        return SafeJSONResponse(content=error_response, status_code=500)

@app.get("/batch/{session_id}/{batch_number}")
async def get_batch(session_id: str, batch_number: int):
    """íŠ¹ì • ë°°ì¹˜ ë°ì´í„° ê°€ì ¸ì˜¤ê¸° - NaN ì•ˆì „ ì²˜ë¦¬"""
    try:
        if session_id not in classifier_instances:
            raise HTTPException(status_code=404, detail="ì„¸ì…˜ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        
        session_data = classifier_instances[session_id]
        classifier = session_data["classifier"]
        batches = session_data["batches"]
        
        if batch_number < 1 or batch_number > len(batches):
            raise HTTPException(status_code=400, detail="ì˜ëª»ëœ ë°°ì¹˜ ë²ˆí˜¸ì…ë‹ˆë‹¤.")
        
        batch_df = batches[batch_number - 1].copy()
        batch_df = batch_df.fillna('')  # NaN ì²˜ë¦¬
        
        month = batch_df['ë…„ì›”'].iloc[0] if 'ë…„ì›”' in batch_df.columns else f"ë°°ì¹˜{batch_number}"
        
        # ë¶„ë¥˜ ìˆ˜í–‰
        if classifier.ensemble_classifier is not None:
            classified_batch = classifier.classify_batch_with_enhanced_model(batch_df, batch_number, str(month))
        else:
            classified_batch = classifier.classify_batch_with_llm(batch_df, batch_number, str(month))
        
        classified_batch = classified_batch.fillna('')  # ì¶”ê°€ NaN ì²˜ë¦¬
        
        confidence_scores = {}
        classifications = {}
        
        for _, row in classified_batch.iterrows():
            try:
                merchant = str(row[classifier.merchant_column])
                category = str(row['ì¹´í…Œê³ ë¦¬'])
                classifications[merchant] = category
                confidence_scores[merchant] = 0.8
            except Exception as e:
                logger.warning(f"í–‰ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
                continue
        
        session_data["current_batch"] = batch_number
        session_data["status"] = "processing"
        
        # DataFrameì„ ì•ˆì „í•˜ê²Œ ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜
        batch_data = clean_for_json(classified_batch.to_dict('records'))
        
        response_data = {
            "batch_number": batch_number,
            "month": str(month),
            "data": batch_data,
            "classifications": classifications,
            "confidence_scores": confidence_scores
        }
        
        return SafeJSONResponse(content=response_data)
        
    except Exception as e:
        logger.error(f"ë°°ì¹˜ ì¡°íšŒ ì˜¤ë¥˜: {e}")
        error_response = {
            "error": "ë°°ì¹˜ ì¡°íšŒ ì‹¤íŒ¨",
            "message": str(e)
        }
        return SafeJSONResponse(content=error_response, status_code=500)

@app.post("/submit-batch")
async def submit_batch(request: BatchProcessRequest):
    """ìˆ˜ì •ëœ ë°°ì¹˜ ì œì¶œ ë° í†µí•© ëª¨ë¸ ì—…ë°ì´íŠ¸ - NaN ì•ˆì „ ì²˜ë¦¬"""
    try:
        session_id = request.session_id
        
        if session_id not in classifier_instances:
            raise HTTPException(status_code=404, detail="ì„¸ì…˜ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        
        session_data = classifier_instances[session_id]
        classifier = session_data["classifier"]
        batches = session_data["batches"]
        
        batch_df = batches[request.batch_number - 1].copy()
        
        # ìˆ˜ì •ì‚¬í•­ ì ìš©
        for merchant, corrected_category in request.corrections.items():
            batch_df.loc[batch_df[classifier.merchant_column] == merchant, 'ì¹´í…Œê³ ë¦¬'] = corrected_category
        
        # sessionsì— ë¶„ë¥˜ ê²°ê³¼ ì €ì¥
        if session_id in sessions:
            if 'classifications' not in sessions[session_id]:
                sessions[session_id]['classifications'] = {}
            sessions[session_id]['classifications'][str(request.batch_number)] = request.corrections
        
        # í†µí•© ëª¨ë¸ ì—…ë°ì´íŠ¸
        classifier.update_training_data_with_corrections(batch_df, request.batch_number)
        performance = classifier.retrain_enhanced_model(request.batch_number)
        
        session_data["status"] = "trained"
        
        response_data = {
            "message": "ë°°ì¹˜ ì²˜ë¦¬ ì™„ë£Œ (í†µí•© ëª¨ë¸ ì—…ë°ì´íŠ¸ë¨)",
            "batch_number": request.batch_number,
            "performance": performance,
            "model_status": classifier.get_model_status(),
            "next_batch_available": request.batch_number < session_data["total_batches"]
        }
        
        return SafeJSONResponse(content=response_data)
        
    except Exception as e:
        logger.error(f"ë°°ì¹˜ ì œì¶œ ì˜¤ë¥˜: {e}")
        error_response = {
            "error": "ë°°ì¹˜ ì œì¶œ ì‹¤íŒ¨",
            "message": str(e)
        }
        return SafeJSONResponse(content=error_response, status_code=500)

@app.get("/session/{session_id}")
async def get_session_info(session_id: str):
    """ì„¸ì…˜ ì •ë³´ ì¡°íšŒ - NaN ì•ˆì „ ì²˜ë¦¬"""
    try:
        if session_id not in classifier_instances:
            raise HTTPException(status_code=404, detail="ì„¸ì…˜ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        
        session_data = classifier_instances[session_id]
        classifier = session_data["classifier"]
        
        response_data = {
            "session_id": session_id,
            "status": session_data["status"],
            "current_batch": session_data["current_batch"],
            "total_batches": session_data["total_batches"],
            "processed_samples": len(classifier.all_training_data),
            "performance_history": classifier.performance_history,
            "created_at": session_data["created_at"].isoformat(),
            "ollama_available": session_data.get("ollama_available", False),
            "model_status": classifier.get_model_status()
        }
        
        return SafeJSONResponse(content=response_data)
        
    except Exception as e:
        logger.error(f"ì„¸ì…˜ ì¡°íšŒ ì˜¤ë¥˜: {e}")
        error_response = {
            "error": "ì„¸ì…˜ ì¡°íšŒ ì‹¤íŒ¨",
            "message": str(e)
        }
        return SafeJSONResponse(content=error_response, status_code=500)

@app.get("/performance/{session_id}")
async def get_performance_history(session_id: str):
    """ì„±ëŠ¥ ì´ë ¥ ì¡°íšŒ - NaN ì•ˆì „ ì²˜ë¦¬"""
    try:
        if session_id not in classifier_instances:
            raise HTTPException(status_code=404, detail="ì„¸ì…˜ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        
        classifier = classifier_instances[session_id]["classifier"]
        return SafeJSONResponse(content=classifier.performance_history)
        
    except Exception as e:
        logger.error(f"ì„±ëŠ¥ ì¡°íšŒ ì˜¤ë¥˜: {e}")
        error_response = {
            "error": "ì„±ëŠ¥ ì¡°íšŒ ì‹¤íŒ¨",
            "message": str(e)
        }
        return SafeJSONResponse(content=error_response, status_code=500)

@app.post("/predict")
async def predict_merchants(request: PredictionRequest):
    """ê°€ë§¹ì ëª… ì˜ˆì¸¡ - NaN ì•ˆì „ ì²˜ë¦¬"""
    try:
        classifier = get_global_classifier()
        
        if classifier.ensemble_classifier is None:
            raise HTTPException(status_code=400, detail="ëª¨ë¸ì´ ì•„ì§ í•™ìŠµë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        
        predictions = {}
        
        for merchant in request.merchant_names:
            keyword_result = classifier.keyword_based_classification(merchant)
            if keyword_result:
                predictions[merchant] = keyword_result
            else:
                try:
                    processed_name = classifier.preprocess_merchant_name(merchant)
                    X = classifier.extract_enhanced_features([processed_name])
                    X_scaled = classifier.scaler.transform(X)
                    
                    probabilities = classifier.ensemble_classifier.predict_proba(X_scaled)[0]
                    predicted_idx = probabilities.argmax()
                    predicted_category = classifier.label_encoder.inverse_transform([predicted_idx])[0]
                    
                    predictions[merchant] = predicted_category
                except Exception:
                    predictions[merchant] = "ê¸°íƒ€"
        
        return SafeJSONResponse(content=predictions)
        
    except Exception as e:
        logger.error(f"ì˜ˆì¸¡ ì˜¤ë¥˜: {e}")
        error_response = {
            "error": "ì˜ˆì¸¡ ì‹¤íŒ¨",
            "message": str(e)
        }
        return SafeJSONResponse(content=error_response, status_code=500)

@app.get("/download/{session_id}")
async def download_results(session_id: str):
    """í˜„ì¬ ì„¸ì…˜ì˜ ë¶„ë¥˜ ê²°ê³¼ ë‹¤ìš´ë¡œë“œ - NaN ì•ˆì „ ì²˜ë¦¬"""
    if session_id not in sessions:
        raise HTTPException(status_code=404, detail="ì„¸ì…˜ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
    
    try:
        session_data = sessions[session_id]
        
        # í˜„ì¬ ì„¸ì…˜ì˜ ì›ë³¸ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
        original_data = session_data.get('original_data')
        if original_data is None or original_data.empty:
            raise HTTPException(status_code=404, detail="ì›ë³¸ ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
        
        # í˜„ì¬ ì„¸ì…˜ì˜ ëª¨ë“  ë¶„ë¥˜ ê²°ê³¼ë¥¼ ë³‘í•©
        classified_data = original_data.copy()
        classified_data = classified_data.fillna('')  # NaN ì²˜ë¦¬
        
        # ê° ë°°ì¹˜ì˜ ë¶„ë¥˜ ê²°ê³¼ë¥¼ ë³‘í•©
        all_corrections = {}
        total_batches = session_data.get('total_batches', 0)
        for batch_num in range(1, total_batches + 1):
            batch_corrections = session_data.get('classifications', {}).get(str(batch_num), {})
            all_corrections.update(batch_corrections)
        
        # ê°€ë§¹ì ëª…ì— ë”°ë¼ ì¹´í…Œê³ ë¦¬ ë§¤í•‘
        merchant_column = session_data.get('merchant_column', 'ê°€ë§¹ì ëª…')
        classified_data['ì¹´í…Œê³ ë¦¬'] = classified_data[merchant_column].map(all_corrections)
        
        # ë¶„ë¥˜ë˜ì§€ ì•Šì€ í•­ëª©ì€ 'ê¸°íƒ€'ë¡œ ì„¤ì •
        classified_data['ì¹´í…Œê³ ë¦¬'] = classified_data['ì¹´í…Œê³ ë¦¬'].fillna('ê¸°íƒ€')
        
        # NaN ê°’ ìµœì¢… ì²˜ë¦¬
        classified_data = classified_data.fillna('')
        
        # ì—‘ì…€ íŒŒì¼ë¡œ ì €ì¥
        output = BytesIO()
        with pd.ExcelWriter(output, engine='openpyxl') as writer:
            classified_data.to_excel(writer, sheet_name='Classification_Result', index=False)
        
        output.seek(0)
        
        # ì˜ë¬¸ íŒŒì¼ëª…ìœ¼ë¡œ ìƒì„±
        original_filename = session_data.get('filename', 'classified_results')
        file_base = original_filename.rsplit('.', 1)[0] if '.' in original_filename else original_filename
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        download_filename = f"{file_base}_classified_{timestamp}.xlsx"
        
        logger.info(f"ë‹¤ìš´ë¡œë“œ ìƒì„± ì™„ë£Œ: {download_filename}, ë¶„ë¥˜ëœ í•­ëª©: {len(all_corrections)}ê°œ")
        
        return Response(
            content=output.getvalue(),
            media_type="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
            headers={
                "Content-Disposition": f"attachment; filename={download_filename}"
            }
        )
        
    except Exception as e:
        logger.error(f"ë‹¤ìš´ë¡œë“œ ì˜¤ë¥˜: {e}")
        raise HTTPException(status_code=500, detail=f"ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {str(e)}")

@app.delete("/session/{session_id}")
async def delete_session(session_id: str):
    """ì„¸ì…˜ ì‚­ì œ - NaN ì•ˆì „ ì²˜ë¦¬"""
    try:
        if session_id in classifier_instances:
            del classifier_instances[session_id]
        
        if session_id in sessions:
            del sessions[session_id]
        
        temp_files = list(temp_dir.glob(f"{session_id}_*"))
        for temp_file in temp_files:
            temp_file.unlink()
        
        response_data = {
            "message": "ì„¸ì…˜ì´ ì‚­ì œë˜ì—ˆìŠµë‹ˆë‹¤. (í†µí•© ëª¨ë¸ì€ ë³´ì¡´ë¨)"
        }
        
        return SafeJSONResponse(content=response_data)
        
    except Exception as e:
        logger.error(f"ì„¸ì…˜ ì‚­ì œ ì˜¤ë¥˜: {e}")
        error_response = {
            "error": "ì„¸ì…˜ ì‚­ì œ ì‹¤íŒ¨",
            "message": str(e)
        }
        return SafeJSONResponse(content=error_response, status_code=500)

@app.get("/categories")
async def get_categories():
    """ì‚¬ìš© ê°€ëŠ¥í•œ ì¹´í…Œê³ ë¦¬ ëª©ë¡ ë°˜í™˜"""
    response_data = {
        "categories": PersistentIncrementalCategorizer.CATEGORIES
    }
    return SafeJSONResponse(content=response_data)

@app.post("/diagnose-model")
async def diagnose_model():
    """í†µí•© ëª¨ë¸ ì§„ë‹¨"""
    try:
        classifier = get_global_classifier()
        
        # ì§„ë‹¨ ë©”ì„œë“œê°€ ìˆëŠ”ì§€ í™•ì¸
        if hasattr(classifier, 'diagnose_ml_model'):
            classifier.diagnose_ml_model()
        else:
            logger.info("ì§„ë‹¨ ë©”ì„œë“œê°€ ì—†ìŠµë‹ˆë‹¤. ê¸°ë³¸ ìƒíƒœë§Œ ë°˜í™˜í•©ë‹ˆë‹¤.")
        
        response_data = {
            "message": "ëª¨ë¸ ì§„ë‹¨ ì™„ë£Œ",
            "model_status": classifier.get_model_status(),
            "check_logs": "ì„œë²„ ë¡œê·¸ë¥¼ í™•ì¸í•˜ì„¸ìš”"
        }
        
        return SafeJSONResponse(content=response_data)
        
    except Exception as e:
        logger.error(f"ëª¨ë¸ ì§„ë‹¨ ì˜¤ë¥˜: {e}")
        error_response = {
            "error": "ëª¨ë¸ ì§„ë‹¨ ì‹¤íŒ¨",
            "message": str(e)
        }
        return SafeJSONResponse(content=error_response, status_code=500)

@app.post("/adjust-threshold")
async def adjust_confidence_threshold(new_threshold: float):
    """ì‹ ë¢°ë„ ì„ê³„ê°’ ìˆ˜ë™ ì¡°ì •"""
    try:
        if not (0.1 <= new_threshold <= 0.9):
            raise HTTPException(status_code=400, detail="ì„ê³„ê°’ì€ 0.1ê³¼ 0.9 ì‚¬ì´ì—¬ì•¼ í•©ë‹ˆë‹¤.")
        
        classifier = get_global_classifier()
        old_threshold = classifier.confidence_threshold
        classifier.confidence_threshold = new_threshold
        
        response_data = {
            "message": "ì‹ ë¢°ë„ ì„ê³„ê°’ ì¡°ì • ì™„ë£Œ",
            "old_threshold": old_threshold,
            "new_threshold": new_threshold,
            "model_status": classifier.get_model_status()
        }
        
        return SafeJSONResponse(content=response_data)
        
    except Exception as e:
        logger.error(f"ì„ê³„ê°’ ì¡°ì • ì˜¤ë¥˜: {e}")
        error_response = {
            "error": "ì„ê³„ê°’ ì¡°ì • ì‹¤íŒ¨",
            "message": str(e)
        }
        return SafeJSONResponse(content=error_response, status_code=500)

@app.get("/stats")
async def get_global_stats():
    """ì „ì²´ í†µê³„ ì¡°íšŒ - NaN ì•ˆì „ ì²˜ë¦¬"""
    try:
        classifier = get_global_classifier()
        
        # ì¹´í…Œê³ ë¦¬ ë¶„í¬ ì•ˆì „ ì²˜ë¦¬
        category_distribution = {}
        if len(classifier.all_training_data) > 0:
            try:
                category_counts = classifier.all_training_data['ì¹´í…Œê³ ë¦¬'].value_counts()
                category_distribution = category_counts.to_dict()
            except Exception as e:
                logger.warning(f"ì¹´í…Œê³ ë¦¬ ë¶„í¬ ê³„ì‚° ì‹¤íŒ¨: {e}")
        
        stats = {
            "model_status": classifier.get_model_status(),
            "total_training_samples": len(classifier.all_training_data),
            "total_batches": len(classifier.performance_history),
            "categories": PersistentIncrementalCategorizer.CATEGORIES,
            "category_distribution": category_distribution,
            "performance_trend": classifier.performance_history[-10:] if classifier.performance_history else []
        }
        
        return SafeJSONResponse(content=stats)
        
    except Exception as e:
        logger.error(f"í†µê³„ ì¡°íšŒ ì˜¤ë¥˜: {e}")
        error_response = {
            "error": "í†µê³„ ì¡°íšŒ ì‹¤íŒ¨",
            "message": str(e)
        }
        return SafeJSONResponse(content=error_response, status_code=500)

def find_available_port(start_port=8080, max_attempts=10):
    """ì‚¬ìš© ê°€ëŠ¥í•œ í¬íŠ¸ ì°¾ê¸°"""
    import socket
    
    for port in range(start_port, start_port + max_attempts):
        try:
            with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:
                s.bind(('localhost', port))
                return port
        except OSError:
            continue
    
    return start_port

if __name__ == "__main__":
    import uvicorn
    
    try:
        port = find_available_port(8080)
        print(f"ğŸš€ ì§€ì†ì  ëª¨ë¸ ì—…ë°ì´íŠ¸ ì„œë²„ê°€ í¬íŠ¸ {port}ì—ì„œ ì‹œì‘ë©ë‹ˆë‹¤...")
        print(f"ğŸ“– API ë¬¸ì„œ: http://localhost:{port}/docs")
        print(f"ğŸŒ í”„ë¡ íŠ¸ì—”ë“œì—ì„œ API_BASE_URLì„ http://localhost:{port}ë¡œ ì„¤ì •í•˜ì„¸ìš”")
        
        # Ollama ìƒíƒœ í™•ì¸
        if check_ollama_connection():
            models = get_available_ollama_models()
            print(f"âœ… Ollama ì—°ê²°ë¨ - ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸: {models}")
        else:
            print("âš ï¸ Ollama ì—°ê²° ì•ˆë¨ - í‚¤ì›Œë“œ ê¸°ë°˜ ë¶„ë¥˜ë§Œ ì‚¬ìš©ë©ë‹ˆë‹¤")
            print("ğŸ’¡ Ollama ì„¤ì¹˜: https://ollama.ai/")
            print("ğŸ’¡ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ: ollama pull llama3")
        
        # í†µí•© ëª¨ë¸ ì´ˆê¸°í™” ë° ìƒíƒœ í™•ì¸
        try:
            classifier = get_global_classifier()
            status = classifier.get_model_status()
            
            print(f"\nğŸ“š í†µí•© ëª¨ë¸ ìƒíƒœ:")
            print(f"   - í•™ìŠµë¨: {status['model_trained']}")
            print(f"   - ëˆ„ì  ìƒ˜í”Œ: {status['total_training_samples']}ê°œ")
            print(f"   - ì²˜ë¦¬ëœ ë°°ì¹˜: {status['total_batches']}ê°œ")
            if status['model_trained']:
                print(f"   - í˜„ì¬ ì •í™•ë„: {status.get('latest_accuracy', 0):.3f}")
                print(f"   - ì‹ ë¢°ë„ ì„ê³„ê°’: {status['confidence_threshold']:.3f}")
            
            if status['total_training_samples'] == 0:
                print("ğŸ“ ê¸°ì¡´ í•™ìŠµëœ ë°ì´í„° ì—†ìŒ. ìƒˆ ë°ì´í„°ë¶€í„° ì‹œì‘í•©ë‹ˆë‹¤.")
            else:
                print("ğŸ¯ ê¸°ì¡´ í•™ìŠµëœ ëª¨ë¸ì„ ê³„ì† ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤.")
                
        except Exception as e:
            print(f"âš ï¸ í†µí•© ëª¨ë¸ í™•ì¸ ì¤‘ ì˜¤ë¥˜: {e}")
        
        print("\nğŸ¯ ìƒˆë¡œìš´ ê¸°ëŠ¥:")
        print("   âœ… ê¸°ì¡´ í•™ìŠµëœ ëª¨ë¸ ìë™ ë¡œë“œ")
        print("   âœ… ìƒˆ ë°ì´í„°ë¡œ ê¸°ì¡´ ëª¨ë¸ ì—…ë°ì´íŠ¸")
        print("   âœ… ì¤‘ë‹¨ í›„ ì¬ì‹œì‘ ê°€ëŠ¥")
        print("   âœ… ëª¨ë¸ ë°±ì—… ë° ë²„ì „ ê´€ë¦¬")
        print("   âœ… ì›”ë³„ ë°°ì¹˜ ìë™ ë¶„í• ")
        print("   âœ… NaN ê°’ ì•ˆì „ ì²˜ë¦¬")
        
        uvicorn.run(
            "main:app",
            host="0.0.0.0", 
            port=port,
            reload=True,
            access_log=True,
            log_level="info"
        )
    except Exception as e:
        print(f"âŒ ì„œë²„ ì‹œì‘ ì‹¤íŒ¨: {e}")
        print("ğŸ’¡ ë‹¤ë¥¸ í„°ë¯¸ë„ì—ì„œ ì‹¤í–‰ ì¤‘ì¸ ì„œë²„ê°€ ìˆëŠ”ì§€ í™•ì¸í•´ì£¼ì„¸ìš”.")