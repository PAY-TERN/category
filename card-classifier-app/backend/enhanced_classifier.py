#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ê³ ë„í™”ëœ ì ì§„ì  ì¬í•™ìŠµ ì¹´ë“œ ë¶„ë¥˜ê¸° (ì§€ì†ì  ëª¨ë¸ ì—…ë°ì´íŠ¸)
- ML ë¶„ë¥˜ ë””ë²„ê¹… ê°œì„ 
- ì¹´ë“œì‚¬ìš© ì›”ë³„ ë°°ì¹˜ ë¶„í• 
- í•˜ë‚˜ì˜ ëª¨ë¸ ì§€ì†ì  ì—…ë°ì´íŠ¸
"""

import pandas as pd
import subprocess
import json
import re
import joblib
from pathlib import Path
from typing import List, Dict, Optional, Tuple, Any
from sklearn.ensemble import RandomForestClassifier, VotingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.metrics import accuracy_score, classification_report
from sklearn.model_selection import cross_val_score
import numpy as np
from collections import Counter
import warnings
import logging
import time
import pickle
from datetime import datetime
import traceback

warnings.filterwarnings('ignore')

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(levelname)s:%(name)s:%(message)s'
)
logger = logging.getLogger(__name__)

class PersistentIncrementalCategorizer:
    """ì§€ì†ì  ëª¨ë¸ ì—…ë°ì´íŠ¸ ë¶„ë¥˜ê¸° (ì›”ë³„ ë°°ì¹˜ ì²˜ë¦¬)"""
    
    CATEGORIES = [
        "ì‹ë¹„", "ì¹´í˜/ê°„ì‹", "í¸ì˜ì /ë§ˆíŠ¸/ì¡í™”", "ìˆ /ìœ í¥", "ì‡¼í•‘", 
        "ì˜í™”/OTT", "ì·¨ë¯¸/ì—¬ê°€", "ì˜ë£Œ/ê±´ê°•", "ì£¼ê±°/í†µì‹ /ê³µê³¼ê¸ˆ", 
        "ë³´í—˜/ì„¸ê¸ˆ/ê¸°íƒ€ê¸ˆìœµ", "ë¯¸ìš©/ë·°í‹°", "êµí†µ/ëŒ€ì¤‘êµí†µ", 
        "ìë™ì°¨/ì£¼ìœ ì†Œ", "ì—¬í–‰/ìˆ™ë°•", "í•­ê³µ", "êµìœ¡", "ìƒí™œ", "ê¸°íƒ€","ê°„í¸ê²°ì œ"
    ]
    
    # ì¹´í…Œê³ ë¦¬ë³„ í‚¤ì›Œë“œ íŒ¨í„´
    CATEGORY_KEYWORDS = {
        "ì‹ë¹„": ["ë§›ì§‘", "ì‹ë‹¹", "íšŸì§‘", "í•œì‹", "ì¤‘ì‹", "ì¼ì‹", "ì–‘ì‹", "ë¶„ì‹", "ì¹˜í‚¨", "í”¼ì", "í–„ë²„ê±°", "ê¹€ë°¥", "êµ­ë°¥", "ì°Œê°œ", "ê³°íƒ•", "ì‚¼ê²¹ì‚´", "ê°ˆë¹„", "ë‹­ê°ˆë¹„", "ì¡±ë°œ", "ë³´ìŒˆ", "ëƒ‰ë©´", "ë¹„ë¹”ë°¥", "ëˆê¹ŒìŠ¤", "ë¼ë©´", "ìš°ë™", "ì§¬ë½•", "ì§œì¥ë©´"],
        "ì¹´í˜/ê°„ì‹": ["ìŠ¤íƒ€ë²…ìŠ¤", "ì´ë””ì•¼", "ì»¤í”¼ë¹ˆ", "íˆ¬ì¸", "ë””ì €íŠ¸", "ë² ì´ì»¤ë¦¬", "ë¹µì§‘", "ë„ë„›", "ì•„ì´ìŠ¤í¬ë¦¼", "ë¹™ìˆ˜", "ë–¡ì§‘", "ê³¼ì", "ì¼€ì´í¬", "ë§ˆì¹´ë¡±", "ì™€í”Œ", "í¬ë¡œí”Œ", "bubble tea", "ë²„ë¸”í‹°", "smoothie", "frappuccino"],
        "í¸ì˜ì /ë§ˆíŠ¸/ì¡í™”": ["gs25", "cu", "ì„¸ë¸ì¼ë ˆë¸", "ë¯¸ë‹ˆìŠ¤í†±", "emart", "ë¡¯ë°ë§ˆíŠ¸", "í™ˆí”ŒëŸ¬ìŠ¤", "ì½”ìŠ¤íŠ¸ì½”", "ë‹¤ì´ì†Œ", "ì•„íŠ¸ë°•ìŠ¤", "ë¬´ì¸ì–‘í’ˆ", "ì˜¬ë¦¬ë¸Œì˜", "ì™“ìŠ¨ìŠ¤", "ë¶€ì¸ ", "ë§ˆì¼“ì»¬ë¦¬", "ì¿ íŒ¡", "í¸ì˜ì ", "ë§ˆíŠ¸", "ìŠˆí¼"],
        "ìˆ /ìœ í¥": ["í˜¸í”„", "ìˆ ì§‘", "ë§¥ì£¼", "ì†Œì£¼", "ì™€ì¸", "ì¹µí…Œì¼", "ë…¸ë˜ë°©", "í´ëŸ½", "íœì…˜", "ë£¸", "ê°€ë¼ì˜¤ì¼€", "í¬ì°¨", "izakaya", "pub", "bar", "beer", "wine", "whiskey", "cocktail"],
        "ì‡¼í•‘": ["ë°±í™”ì ", "ì•„ìš¸ë ›", "ì˜¨ë¼ì¸", "ì‡¼í•‘ëª°", "ì˜ë¥˜", "ì‹ ë°œ", "ê°€ë°©", "ì•¡ì„¸ì„œë¦¬", "ë¸Œëœë“œ", "íŒ¨ì…˜", "ìœ ë‹ˆí´ë¡œ", "ìë¼", "h&m", "gap", "ë‚˜ì´í‚¤", "ì•„ë””ë‹¤ìŠ¤", "ëª…í’ˆ", "luxury"],
        "ì˜í™”/OTT": ["cgv", "ë¡¯ë°ì‹œë„¤ë§ˆ", "ë©”ê°€ë°•ìŠ¤", "ì˜í™”", "ë„·í”Œë¦­ìŠ¤", "ë””ì¦ˆë‹ˆ", "ì™“ì± ", "ì›¨ì´ë¸Œ", "cinema", "movie", "netflix", "disney+", "youtube", "spotify", "apple music"],
        "ì·¨ë¯¸/ì—¬ê°€": ["pcë°©", "ê²Œì„", "ë³¼ë§", "ë‹¹êµ¬", "ê³¨í”„", "ìˆ˜ì˜", "í—¬ìŠ¤", "ìš”ê°€", "ë…ì„œì‹¤", "ìŠ¤í¬ì¸ ", "ì°œì§ˆë°©", "ì‚¬ìš°ë‚˜", "spa", "ë§ˆì‚¬ì§€", "ê²Œì„ì„¼í„°", "ì˜¤ë½ì‹¤", "ë…¸ë˜ë°©", "escape room"],
        "ì˜ë£Œ/ê±´ê°•": ["ë³‘ì›", "ì•½êµ­", "ì¹˜ê³¼", "í•œì˜ì›", "ì•ˆê³¼", "í”¼ë¶€ê³¼", "ì •í˜•ì™¸ê³¼", "ë‚´ê³¼", "ê²€ì§„", "ì˜ë£Œ", "pharmacy", "hospital", "clinic", "dental", "medical", "health", "medicine"],
        "ì£¼ê±°/í†µì‹ /ê³µê³¼ê¸ˆ": ["ì „ê¸°ìš”ê¸ˆ", "ê°€ìŠ¤ìš”ê¸ˆ", "ìˆ˜ë„ìš”ê¸ˆ", "ê´€ë¦¬ë¹„", "í†µì‹ ë¹„", "ì¸í„°ë„·", "íœ´ëŒ€í°", "ì „í™”ìš”ê¸ˆ", "ì„ëŒ€ë£Œ", "ì›”ì„¸", "ì „ì„¸", "kt", "skt", "lgìœ í”ŒëŸ¬ìŠ¤", "utility", "rent"],
        "ë³´í—˜/ì„¸ê¸ˆ/ê¸°íƒ€ê¸ˆìœµ": ["ë³´í—˜", "ì„¸ê¸ˆ", "êµ­ì„¸ì²­", "ì‹œì²­", "êµ¬ì²­", "ì€í–‰", "ì¹´ë“œ", "ëŒ€ì¶œ", "ì ê¸ˆ", "í€ë“œ", "íˆ¬ì", "ì£¼ì‹", "insurance", "tax", "bank", "loan", "investment", "stock"],
        "ë¯¸ìš©/ë·°í‹°": ["ë¯¸ìš©ì‹¤", "í—¤ì–´ìƒµ", "ë„¤ì¼ìƒµ", "í”¼ë¶€ê´€ë¦¬", "ë§ˆì‚¬ì§€", "í™”ì¥í’ˆ", "ì—ìŠ¤í…Œí‹±", "ìŠ¤íŒŒ", "beauty", "salon", "nail", "cosmetic", "skincare", "makeup", "perfume"],
        "êµí†µ/ëŒ€ì¤‘êµí†µ": ["ì§€í•˜ì² ", "ë²„ìŠ¤", "íƒì‹œ", "ì¹´ì¹´ì˜¤íƒì‹œ", "ìš°ë²„", "ê¸°ì°¨", "ê³ ì†ë²„ìŠ¤", "ì‹œì™¸ë²„ìŠ¤", "êµí†µì¹´ë“œ", "subway", "bus", "taxi", "train", "uber", "grab", "transport"],
        "ìë™ì°¨/ì£¼ìœ ì†Œ": ["ì£¼ìœ ì†Œ", "gsì¹¼í…ìŠ¤", "skì—ë„ˆì§€", "í˜„ëŒ€ì˜¤ì¼ë±…í¬", "s-oil", "ì •ë¹„ì†Œ", "ì„¸ì°¨", "íƒ€ì´ì–´", "ìë™ì°¨", "gas station", "oil", "car wash", "tire", "auto", "vehicle"],
        "ì—¬í–‰/ìˆ™ë°•": ["í˜¸í…”", "ëª¨í…”", "íœì…˜", "ë¦¬ì¡°íŠ¸", "ì—ì–´ë¹„ì•¤ë¹„", "ê²ŒìŠ¤íŠ¸í•˜ìš°ìŠ¤", "ì—¬í–‰ì‚¬", "í•­ê³µê¶Œ", "ê¸°ì°¨í‘œ", "hotel", "motel", "resort", "airbnb", "travel", "booking", "agoda"],
        "í•­ê³µ": ["ëŒ€í•œí•­ê³µ", "ì•„ì‹œì•„ë‚˜", "ì œì£¼í•­ê³µ", "ì§„ì—ì–´", "í‹°ì›¨ì´", "ì´ìŠ¤íƒ€í•­ê³µ", "ê³µí•­", "í•­ê³µ", "airline", "airport", "flight", "aviation", "korean air", "asiana"],
        "êµìœ¡": ["í•™ì›", "ê³¼ì™¸", "ì˜¨ë¼ì¸ê°•ì˜", "êµìœ¡ë¹„", "í•™ë¹„", "êµì¬", "ë¬¸êµ¬ì ", "í•™ìš©í’ˆ", "ìœ ì¹˜ì›", "ì–´í•™ì›", "academy", "education", "school", "university", "course", "tuition"],
        "ìƒí™œ": ["ì„¸íƒì†Œ", "ìˆ˜ì„ ì§‘", "ì—´ì‡ ", "ì¸ì‡„", "ë³µì‚¬", "íƒë°°", "ìš°ì²´êµ­", "ë™ì‚¬ë¬´ì†Œ", "ë¯¼ì›", "ìƒí™œìš©í’ˆ", "laundry", "post office", "copy", "print", "delivery", "repair"]
    }
    
    def __init__(self, merchant_column: str = 'ê°€ë§¹ì ëª…', date_column: str = 'ì´ìš©ì¼ì'):
        """ì´ˆê¸°í™” - í•˜ë‚˜ì˜ ëª¨ë¸ë§Œ ì§€ì†ì ìœ¼ë¡œ ì—…ë°ì´íŠ¸"""
        self.merchant_column = merchant_column
        self.date_column = date_column
        
        # ëª¨ë¸ëª…ì€ ê³ ì • - í•˜ë‚˜ì˜ ëª¨ë¸ë§Œ ì‚¬ìš©
        self.model_name = "unified_model"
        
        # ëª¨ë¸ ì €ì¥ ë””ë ‰í† ë¦¬
        self.model_dir = Path('models')
        self.model_dir.mkdir(exist_ok=True)
        
        # ë°ì´í„° ì €ì¥ ë””ë ‰í† ë¦¬
        self.data_dir = Path('training_data')
        self.data_dir.mkdir(exist_ok=True)
        
        # ì•™ìƒë¸” ëª¨ë¸ êµ¬ì„±
        self.ensemble_classifier = None
        self.vectorizer = None
        self.char_vectorizer = None
        self.label_encoder = None
        self.scaler = StandardScaler()
        
        # í•™ìŠµ ë°ì´í„° ê´€ë¦¬
        self.all_training_data = pd.DataFrame()
        self.performance_history = []
        self.current_batch = 0
        
        # ì‹ ë¢°ë„ ì„ê³„ê°’ ë™ì  ì¡°ì • (ì´ˆê¸°ê°’ì„ ë‚®ê²Œ ì„¤ì •)
        self.confidence_threshold = 0.4  # 0.7ì—ì„œ 0.5ë¡œ ë‚®ì¶¤
        self.min_confidence_threshold = 0.3
        
        # í‚¤ì›Œë“œ ê¸°ë°˜ ë¶„ë¥˜ê¸°
        self.keyword_patterns = self._compile_keyword_patterns()
        
        # Ollama ì„¤ì •
        self.ollama_model = "llama3"
        self.ollama_available = self._check_ollama_connection()
        
        # ì´ˆê¸°í™” ì‹œ ê¸°ì¡´ ëª¨ë¸ ìë™ ë¡œë“œ ì‹œë„
        self.auto_load_existing_model()
    
    def create_monthly_batches(self, df: pd.DataFrame) -> List[pd.DataFrame]:
        """ì›”ë³„ë¡œ ë°°ì¹˜ ìƒì„±"""
        logger.info("ğŸ“¦ ì›”ë³„ ë°°ì¹˜ ìƒì„± ì¤‘...")
        
        # ë‚ ì§œ íŒŒì‹±
        df = self.parse_date_column(df)
        
        # ìœ íš¨í•œ ë‚ ì§œê°€ ìˆëŠ” ë°ì´í„°ë§Œ ì‚¬ìš©
        valid_df = df[df['ë…„ì›”'].notna()].copy()
        
        if len(valid_df) == 0:
            logger.error("âŒ ìœ íš¨í•œ ë‚ ì§œ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return []
        
        # ì›”ë³„ ê·¸ë£¹í™”
        monthly_groups = valid_df.groupby('ë…„ì›”')
        batches = []
        
        # ì›”ë³„ë¡œ ì •ë ¬í•´ì„œ ë°°ì¹˜ ìƒì„±
        sorted_months = sorted(monthly_groups.groups.keys())
        
        for month in sorted_months:
            month_data = monthly_groups.get_group(month)
            batches.append(month_data.copy())
            logger.info(f"   ğŸ“… {month}: {len(month_data)}ê°œ ê±°ë˜")
        
        logger.info(f"ğŸ“¦ ì´ {len(batches)}ê°œ ì›”ë³„ ë°°ì¹˜ ìƒì„±")
        return batches
    
    def load_excel_data(self, excel_path: str) -> pd.DataFrame:
        """ì—‘ì…€ ë°ì´í„° ë¡œë“œ"""
        logger.info(f"ğŸ“ ì—‘ì…€ íŒŒì¼ ë¡œë“œ: {excel_path}")
        df = pd.read_excel(excel_path)
        logger.info(f"ğŸ“Š ì „ì²´ ë°ì´í„°: {len(df)}ê°œ í–‰")
        
        df_filtered = df[df[self.merchant_column].notna()].copy()
        logger.info(f"ğŸ“Š ìœ íš¨í•œ ë°ì´í„°: {len(df_filtered)}ê°œ í–‰")
        
        return df_filtered
    
    def _compile_keyword_patterns(self) -> Dict[str, List[re.Pattern]]:
        """í‚¤ì›Œë“œ íŒ¨í„´ ì»´íŒŒì¼"""
        patterns = {}
        for category, keywords in self.CATEGORY_KEYWORDS.items():
            patterns[category] = [re.compile(keyword, re.IGNORECASE) for keyword in keywords]
        return patterns
    
    def _check_ollama_connection(self) -> bool:
        """Ollama ì—°ê²° ìƒíƒœ í™•ì¸"""
        try:
            result = subprocess.run(
                ['ollama', 'list'],
                capture_output=True,
                timeout=5
            )
            return result.returncode == 0
        except Exception as e:
            logger.warning(f"Ollama ì—°ê²° í™•ì¸ ì‹¤íŒ¨: {e}")
            return False
    
    def auto_load_existing_model(self):
        """ì´ˆê¸°í™” ì‹œ ê¸°ì¡´ ëª¨ë¸ ìë™ ë¡œë“œ"""
        logger.info("ğŸ” ê¸°ì¡´ í•™ìŠµëœ ëª¨ë¸ ê²€ìƒ‰ ì¤‘...")
        
        # ìµœì‹  ëª¨ë¸ íŒŒì¼ í™•ì¸
        latest_model_path = self.model_dir / f'{self.model_name}_latest.pkl'
        
        if latest_model_path.exists():
            try:
                logger.info(f"âœ… ê¸°ì¡´ ëª¨ë¸ ë°œê²¬: {latest_model_path}")
                self.load_complete_model()
                logger.info(f"ğŸ¯ ê¸°ì¡´ ëª¨ë¸ ë¡œë“œ ì™„ë£Œ! ëˆ„ì  í•™ìŠµ ë°ì´í„°: {len(self.all_training_data)}ê°œ")
                return True
            except Exception as e:
                logger.warning(f"âš ï¸ ê¸°ì¡´ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
        
        logger.info("ğŸ“ ê¸°ì¡´ ëª¨ë¸ ì—†ìŒ. ìƒˆë¡œìš´ ëª¨ë¸ë¡œ ì‹œì‘í•©ë‹ˆë‹¤.")
        return False
    def save_complete_model(self, batch_num: int = None):
        """ëª¨ë¸ê³¼ ëª¨ë“  ê´€ë ¨ ë°ì´í„° ì™„ì „ ì €ì¥ - ì™„ì „ ìˆ˜ì •ë¨"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # performance_history ì™„ì „ ì •ì œ
        cleaned_performance_history = []
        for i, perf in enumerate(self.performance_history):
            if isinstance(perf, dict):
                # í•„ìˆ˜ í•„ë“œê°€ ëª¨ë‘ ìˆëŠ”ì§€ í™•ì¸í•˜ê³  ì •ì œ
                cleaned_perf = {
                    'batch': int(perf.get('batch', i + 1)),
                    'train_accuracy': float(perf.get('train_accuracy', 0.0)),
                    'cv_mean': float(perf.get('cv_mean', 0.0)),
                    'cv_std': float(perf.get('cv_std', 0.0)),
                    'training_samples': int(perf.get('training_samples', 0)),
                    'categories': int(perf.get('categories', 0)),
                    'confidence_threshold': float(perf.get('confidence_threshold', 0.5)),
                    'timestamp': str(perf.get('timestamp', datetime.now().isoformat()))
                }
                
                # DataFrameì´ë‚˜ ë³µì¡í•œ ê°ì²´ê°€ ì—†ëŠ”ì§€ í™•ì¸
                valid_entry = True
                for key, value in cleaned_perf.items():
                    if isinstance(value, (pd.DataFrame, pd.Series)):
                        logger.warning(f"âš ï¸ ì„±ëŠ¥ ë°ì´í„°ì—ì„œ pandas ê°ì²´ ê°ì§€, í•´ë‹¹ í•­ëª© ì œì™¸")
                        valid_entry = False
                        break
                
                if valid_entry:
                    cleaned_performance_history.append(cleaned_perf)
        
        logger.info(f"ğŸ“Š ì„±ëŠ¥ íˆìŠ¤í† ë¦¬ ì •ì œ: {len(self.performance_history)} -> {len(cleaned_performance_history)}ê°œ")
        
        # ëª¨ë¸ ìƒíƒœ ë”•ì…”ë„ˆë¦¬ (ì •ì œëœ ë°ì´í„°ë§Œ í¬í•¨)
        model_state = {
            'ensemble_classifier': self.ensemble_classifier,
            'vectorizer': self.vectorizer,
            'char_vectorizer': self.char_vectorizer,
            'label_encoder': self.label_encoder,
            'scaler': self.scaler,
            'confidence_threshold': float(self.confidence_threshold),
            'min_confidence_threshold': float(self.min_confidence_threshold),
            'performance_history': cleaned_performance_history,  # ì™„ì „ ì •ì œëœ íˆìŠ¤í† ë¦¬
            'current_batch': int(self.current_batch),
            'ollama_model': str(self.ollama_model),
            'model_name': str(self.model_name),
            'categories': list(self.CATEGORIES),
            'saved_at': timestamp,
            'merchant_column': str(self.merchant_column),
            'date_column': str(self.date_column)
        }
        
        # ìµœì‹  ëª¨ë¸ ì €ì¥
        latest_model_path = self.model_dir / f'{self.model_name}_latest.pkl'
        with open(latest_model_path, 'wb') as f:
            pickle.dump(model_state, f)
        
        # ë°°ì¹˜ë³„ ë°±ì—… ì €ì¥
        if batch_num is not None and isinstance(batch_num, int):
            backup_model_path = self.model_dir / f'{self.model_name}_batch_{batch_num}.pkl'
            with open(backup_model_path, 'wb') as f:
                pickle.dump(model_state, f)
            
            logger.info(f"ğŸ’¾ ëª¨ë¸ ì €ì¥ ì™„ë£Œ: ë°°ì¹˜ {batch_num}")
        
        # í•™ìŠµ ë°ì´í„° ì €ì¥
        if len(self.all_training_data) > 0:
            latest_data_path = self.data_dir / f'{self.model_name}_training_data.xlsx'
            self.all_training_data.to_excel(latest_data_path, index=False)
        
        logger.info(f"âœ… ì™„ì „ ì €ì¥ ì™„ë£Œ: {latest_model_path}")
        
        # ì •ì œëœ ë°ì´í„°ë¡œ ë©”ëª¨ë¦¬ìƒ performance_history ì—…ë°ì´íŠ¸
        self.performance_history = cleaned_performance_history
        
    def load_complete_model(self, batch_num: int = None):
            """ì €ì¥ëœ ëª¨ë¸ê³¼ ëª¨ë“  ê´€ë ¨ ë°ì´í„° ì™„ì „ ë¡œë“œ - ì™„ì „ ìˆ˜ì •ë¨"""
            try:
                # ë¡œë“œí•  ëª¨ë¸ íŒŒì¼ ê²°ì •
                if batch_num:
                    model_path = self.model_dir / f'{self.model_name}_batch_{batch_num}.pkl'
                else:
                    model_path = self.model_dir / f'{self.model_name}_latest.pkl'
                
                # ëª¨ë¸ ìƒíƒœ ë¡œë“œ
                if not model_path.exists():
                    raise FileNotFoundError(f"ëª¨ë¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {model_path}")
                
                with open(model_path, 'rb') as f:
                    model_state = pickle.load(f)
                
                # ëª¨ë¸ ìƒíƒœ ë³µì›
                self.ensemble_classifier = model_state['ensemble_classifier']
                self.vectorizer = model_state['vectorizer']
                self.char_vectorizer = model_state['char_vectorizer']
                self.label_encoder = model_state['label_encoder']
                self.scaler = model_state['scaler']
                self.confidence_threshold = model_state.get('confidence_threshold', 0.5)

                # ì„ê³„ê°’ ìƒí•œ ì œí•œ
                if self.confidence_threshold > 0.7:
                    logger.warning(f"âš ï¸ ë¶ˆí•©ë¦¬í•˜ê²Œ ë†’ì€ ì‹ ë¢°ë„ ì„ê³„ê°’ ê°ì§€: {self.confidence_threshold} â†’ 0.7ë¡œ ì¡°ì •")
                    self.confidence_threshold = 0.7
                        
                self.min_confidence_threshold = model_state.get('min_confidence_threshold', 0.3)
                
                # performance_history ì™„ì „ ì •ì œ ë° ì¬êµ¬ì„±
                raw_history = model_state.get('performance_history', [])
                self.performance_history = []
                
                for i, perf in enumerate(raw_history):
                    if isinstance(perf, dict):
                        # ìƒˆë¡œìš´ ì •ì œëœ ì„±ëŠ¥ ë”•ì…”ë„ˆë¦¬ ìƒì„±
                        cleaned_perf = {
                            'batch': perf.get('batch', i + 1),  # batchê°€ ì—†ìœ¼ë©´ ìˆœì„œëŒ€ë¡œ ë²ˆí˜¸ ë¶€ì—¬
                            'train_accuracy': float(perf.get('train_accuracy', 0.0)),
                            'cv_mean': float(perf.get('cv_mean', 0.0)),
                            'cv_std': float(perf.get('cv_std', 0.0)),
                            'training_samples': int(perf.get('training_samples', 0)),
                            'categories': int(perf.get('categories', 0)),
                            'confidence_threshold': float(perf.get('confidence_threshold', 0.5)),
                            'timestamp': perf.get('timestamp', datetime.now().isoformat())
                        }
                        
                        # DataFrameì´ë‚˜ ê¸°íƒ€ ë³µì¡í•œ ê°ì²´ëŠ” ì œì™¸
                        valid_perf = True
                        for key, value in cleaned_perf.items():
                            if isinstance(value, pd.DataFrame):
                                logger.warning(f"   âš ï¸ ì„±ëŠ¥ ë°ì´í„°ì—ì„œ DataFrame ë°œê²¬, í•­ëª© ì œì™¸: {i}")
                                valid_perf = False
                                break
                        
                        if valid_perf:
                            self.performance_history.append(cleaned_perf)
                            logger.debug(f"   âœ… ì„±ëŠ¥ ë°ì´í„° ì •ì œ ì™„ë£Œ: ë°°ì¹˜ {cleaned_perf['batch']}")
                
                self.current_batch = model_state.get('current_batch', 0)
                
                # í•™ìŠµ ë°ì´í„° ë¡œë“œ
                data_path = self.data_dir / f'{self.model_name}_training_data.xlsx'
                if data_path.exists():
                    self.all_training_data = pd.read_excel(data_path)
                    logger.info(f"ğŸ“Š í•™ìŠµ ë°ì´í„° ë¡œë“œ: {len(self.all_training_data)}ê°œ")
                else:
                    self.all_training_data = pd.DataFrame()
                
                # ì •ì œëœ ë°ì´í„°ë¡œ ì¦‰ì‹œ ì¬ì €ì¥ (í–¥í›„ ë¬¸ì œ ë°©ì§€)
                logger.info("ğŸ”§ ì •ì œëœ ë°ì´í„°ë¡œ ëª¨ë¸ ì¬ì €ì¥ ì¤‘...")
                self.save_complete_model()
                
                logger.info(f"âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ!")
                logger.info(f"   - ì´ ë°°ì¹˜: {len(self.performance_history)}ê°œ")
                logger.info(f"   - ëˆ„ì  ìƒ˜í”Œ: {len(self.all_training_data)}ê°œ")
                logger.info(f"   - í˜„ì¬ ì •í™•ë„: {self.performance_history[-1]['train_accuracy']:.3f}" if self.performance_history else "   - ì •í™•ë„: ì•„ì§ ì—†ìŒ")
                
                return True
                
            except Exception as e:
                logger.error(f"âŒ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
                logger.error(f"   ìƒì„¸ ì˜¤ë¥˜: {traceback.format_exc()}")
                return False
    # enhanced_classifier.pyì˜ parse_date_column í•¨ìˆ˜ë¥¼ ë‹¤ìŒê³¼ ê°™ì´ ìˆ˜ì •

    def parse_date_column(self, df: pd.DataFrame) -> pd.DataFrame:
        """ë‚ ì§œ ì»¬ëŸ¼ íŒŒì‹± (ë‹¤ì–‘í•œ í˜•ì‹ ì§€ì›) - ë””ë²„ê¹… ê°•í™”"""
        df = df.copy()
        
        if self.date_column not in df.columns:
            raise ValueError(f"ë‚ ì§œ ì»¬ëŸ¼ '{self.date_column}'ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        
        logger.info(f"ğŸ” ë‚ ì§œ ì»¬ëŸ¼ '{self.date_column}' ì›ë³¸ ë°ì´í„° ìƒ˜í”Œ:")
        sample_data = df[self.date_column].dropna().head(10)
        for i, value in enumerate(sample_data):
            logger.info(f"   {i+1}. {repr(value)} (íƒ€ì…: {type(value).__name__})")
        
        try:
            # 1ì°¨: pandas ê¸°ë³¸ íŒŒì‹± ì‹œë„
            logger.info("ğŸ“… 1ì°¨: pandas ê¸°ë³¸ ë‚ ì§œ íŒŒì‹± ì‹œë„...")
            df_temp = df.copy()
            df_temp[self.date_column] = pd.to_datetime(df_temp[self.date_column], errors='coerce')
            
            # ì„±ê³µì ìœ¼ë¡œ íŒŒì‹±ëœ ë°ì´í„° í™•ì¸
            valid_dates = df_temp[self.date_column].dropna()
            if len(valid_dates) > 0:
                logger.info(f"âœ… pandas íŒŒì‹± ì„±ê³µ: {len(valid_dates)}ê°œ / {len(df)}ê°œ")
                logger.info(f"   ë‚ ì§œ ë²”ìœ„: {valid_dates.min()} ~ {valid_dates.max()}")
                
                # 1970ë…„ëŒ€ ë°ì´í„°ê°€ ë§ìœ¼ë©´ ì—‘ì…€ ìˆ«ì ë‚ ì§œì¼ ê°€ëŠ¥ì„±
                dates_1970 = valid_dates[valid_dates.dt.year == 1970]
                if len(dates_1970) > len(valid_dates) * 0.5:
                    logger.warning(f"âš ï¸ 1970ë…„ ë‚ ì§œê°€ {len(dates_1970)}ê°œ ê°ì§€ - ì—‘ì…€ ìˆ«ì ë‚ ì§œ ë³€í™˜ ì‹œë„")
                    # ì—‘ì…€ ìˆ«ì ë‚ ì§œ ë³€í™˜ ì‹œë„
                    df = self._convert_excel_serial_dates(df)
                else:
                    df = df_temp
            else:
                logger.warning("âš ï¸ pandas ê¸°ë³¸ íŒŒì‹± ì‹¤íŒ¨ - ìˆ˜ë™ ë³€í™˜ ì‹œë„")
                df = self._manual_date_conversion(df)
                
        except Exception as e:
            logger.error(f"âŒ ë‚ ì§œ íŒŒì‹± ì˜¤ë¥˜: {e}")
            df = self._manual_date_conversion(df)
        
        # ìµœì¢… ê²°ê³¼ í™•ì¸
        valid_count = df[self.date_column].notna().sum()
        failed_count = len(df) - valid_count
        
        if failed_count > 0:
            logger.warning(f"âš ï¸ ë‚ ì§œ íŒŒì‹± ì‹¤íŒ¨: {failed_count}ê°œ í–‰")
        
        # ì—°-ì›” ì»¬ëŸ¼ ì¶”ê°€
        df['ë…„ì›”'] = df[self.date_column].dt.strftime('%Y-%m')
        
        # ì›”ë³„ ë¶„í¬ í™•ì¸
        month_distribution = df['ë…„ì›”'].value_counts().sort_index()
        logger.info(f"ğŸ“Š ì›”ë³„ ë¶„í¬:")
        for month, count in month_distribution.head(10).items():
            logger.info(f"   {month}: {count}ê°œ")
        
        logger.info(f"ğŸ“… ë‚ ì§œ íŒŒì‹± ì™„ë£Œ: {valid_count}ê°œ í–‰")
        
        return df

    def _convert_excel_serial_dates(self, df: pd.DataFrame) -> pd.DataFrame:
        """ì—‘ì…€ ì‹œë¦¬ì–¼ ë‚ ì§œ ë³€í™˜"""
        logger.info("ğŸ”„ ì—‘ì…€ ì‹œë¦¬ì–¼ ë‚ ì§œ ë³€í™˜ ì‹œë„...")
        df_temp = df.copy()
        
        def convert_excel_serial(value):
            """ì—‘ì…€ ì‹œë¦¬ì–¼ ë‚ ì§œë¥¼ datetimeìœ¼ë¡œ ë³€í™˜"""
            try:
                if pd.isna(value):
                    return None
                
                # ìˆ«ìì¸ ê²½ìš° ì—‘ì…€ ì‹œë¦¬ì–¼ ë‚ ì§œë¡œ ê°„ì£¼
                if isinstance(value, (int, float)):
                    # ì—‘ì…€ ê¸°ì¤€: 1900ë…„ 1ì›” 1ì¼ = 1
                    # pandas ê¸°ì¤€: 1899ë…„ 12ì›” 30ì¼ = 0
                    excel_epoch = pd.Timestamp('1899-12-30')
                    days_to_add = pd.Timedelta(days=int(value))
                    return excel_epoch + days_to_add
                
                # ë¬¸ìì—´ì¸ ê²½ìš° ê·¸ëŒ€ë¡œ íŒŒì‹± ì‹œë„
                return pd.to_datetime(value, errors='coerce')
                
            except Exception:
                return None
        
        df_temp[self.date_column] = df_temp[self.date_column].apply(convert_excel_serial)
        
        # ë³€í™˜ ê²°ê³¼ í™•ì¸
        valid_converted = df_temp[self.date_column].dropna()
        if len(valid_converted) > 0:
            logger.info(f"âœ… ì—‘ì…€ ë³€í™˜ ì„±ê³µ: {len(valid_converted)}ê°œ")
            logger.info(f"   ë³€í™˜ í›„ ë‚ ì§œ ë²”ìœ„: {valid_converted.min()} ~ {valid_converted.max()}")
            return df_temp
        else:
            logger.warning("âš ï¸ ì—‘ì…€ ë³€í™˜ ì‹¤íŒ¨")
            return df

    def _manual_date_conversion(self, df: pd.DataFrame) -> pd.DataFrame:
        """ìˆ˜ë™ ë‚ ì§œ ë³€í™˜ (ë‹¤ì–‘í•œ í˜•ì‹ ì§€ì›)"""
        logger.info("ğŸ”§ ìˆ˜ë™ ë‚ ì§œ ë³€í™˜ ì‹œë„...")
        df_temp = df.copy()
        
        def parse_date_manual(value):
            """ë‹¤ì–‘í•œ ë‚ ì§œ í˜•ì‹ ìˆ˜ë™ íŒŒì‹±"""
            if pd.isna(value):
                return None
            
            value_str = str(value).strip()
            
            # íŒ¨í„´ë“¤ ì‹œë„
            patterns = [
                r'(\d{4})[/-](\d{1,2})[/-](\d{1,2})',  # YYYY-MM-DD, YYYY/MM/DD
                r'(\d{1,2})[/-](\d{1,2})[/-](\d{4})',  # MM/DD/YYYY, DD/MM/YYYY
                r'(\d{4})(\d{2})(\d{2})',              # YYYYMMDD
                r'(\d{4})\.(\d{1,2})\.(\d{1,2})',      # YYYY.MM.DD
            ]
            
            for pattern in patterns:
                match = re.match(pattern, value_str)
                if match:
                    groups = match.groups()
                    try:
                        if len(groups[0]) == 4:  # ì²« ë²ˆì§¸ê°€ ì—°ë„
                            year, month, day = int(groups[0]), int(groups[1]), int(groups[2])
                        else:  # ì²« ë²ˆì§¸ê°€ ì›” ë˜ëŠ” ì¼
                            month, day, year = int(groups[0]), int(groups[1]), int(groups[2])
                        
                        return pd.Timestamp(year=year, month=month, day=day)
                    except ValueError:
                        continue
            
            # ëª¨ë“  íŒ¨í„´ ì‹¤íŒ¨ ì‹œ None ë°˜í™˜
            return None
        
        df_temp[self.date_column] = df_temp[self.date_column].apply(parse_date_manual)
        
        valid_manual = df_temp[self.date_column].dropna()
        if len(valid_manual) > 0:
            logger.info(f"âœ… ìˆ˜ë™ ë³€í™˜ ì„±ê³µ: {len(valid_manual)}ê°œ")
            logger.info(f"   ìˆ˜ë™ ë³€í™˜ í›„ ë‚ ì§œ ë²”ìœ„: {valid_manual.min()} ~ {valid_manual.max()}")
            return df_temp
        else:
            logger.error("âŒ ëª¨ë“  ë‚ ì§œ ë³€í™˜ ë°©ë²• ì‹¤íŒ¨")
            return df

    # ë˜í•œ ë°±ì—”ë“œì˜ ì „ì²˜ë¦¬ í•¨ìˆ˜ë„ ìˆ˜ì •
    def convert_excel_date(value):
        """ì—‘ì…€ ì‹œë¦¬ì–¼ ë‚ ì§œë¥¼ ì‹¤ì œ ë‚ ì§œë¡œ ë³€í™˜ (ê°œì„ ëœ ë²„ì „)"""
        if pd.isna(value):
            return None
        
        # ì´ë¯¸ datetime ê°ì²´ì¸ ê²½ìš°
        if isinstance(value, datetime):
            return value.strftime('%Y-%m-%d')
        
        # ìˆ«ìì¸ ê²½ìš° - ì—‘ì…€ ì‹œë¦¬ì–¼ ë‚ ì§œ ì²˜ë¦¬
        if isinstance(value, (int, float)):
            try:
                # í•©ë¦¬ì ì¸ ë²”ìœ„ í™•ì¸ (1900ë…„~2100ë…„ ì‚¬ì´)
                if 1 <= value <= 73048:  # 1900-01-01 ~ 2099-12-31
                    # ì—‘ì…€ ê¸°ì¤€ì : 1899-12-30
                    excel_epoch = datetime(1899, 12, 30)
                    converted_date = excel_epoch + timedelta(days=int(value))
                    return converted_date.strftime('%Y-%m-%d')
                else:
                    return str(value)  # ë²”ìœ„ ë°–ì´ë©´ ë¬¸ìì—´ë¡œ ìœ ì§€
            except Exception as e:
                logger.warning(f"ìˆ«ì ë‚ ì§œ ë³€í™˜ ì‹¤íŒ¨: {value} -> {e}")
                return str(value)
        
        # ë¬¸ìì—´ì¸ ê²½ìš°
        if isinstance(value, str):
            # ì´ë¯¸ ì˜¬ë°”ë¥¸ í˜•ì‹ì¸ ê²½ìš°
            if re.match(r'\d{4}-\d{2}-\d{2}', value):
                return value
            
            # ë‹¤ì–‘í•œ í˜•ì‹ ì‹œë„
            try:
                parsed_date = pd.to_datetime(value, errors='coerce')
                if not pd.isna(parsed_date):
                    return parsed_date.strftime('%Y-%m-%d')
            except:
                pass
        
        return str(value)

    # ë‚ ì§œ ì»¬ëŸ¼ ê°ì§€ í•¨ìˆ˜ë„ ê°œì„ 
    def is_likely_excel_date(series, threshold=0.3):
        """ì‹œë¦¬ì¦ˆê°€ ì—‘ì…€ ë‚ ì§œ ì»¬ëŸ¼ì¸ì§€ íŒë‹¨ (ê°œì„ ëœ ë²„ì „)"""
        if series.dtype not in ['int64', 'float64', 'object']:
            return False
            
        numeric_values = pd.to_numeric(series, errors='coerce').dropna()
        if len(numeric_values) == 0:
            return False
        
        # ì—‘ì…€ ë‚ ì§œ ë²”ìœ„ í™•ì¸ (1900ë…„~2030ë…„)
        excel_range_values = numeric_values[(numeric_values >= 1) & (numeric_values <= 47483)]
        recent_date_values = numeric_values[(numeric_values >= 36526) & (numeric_values <= 47483)]  # 2000-2030
        
        recent_ratio = len(recent_date_values) / len(numeric_values)
        total_ratio = len(excel_range_values) / len(numeric_values)
        
        logger.info(f"   ë‚ ì§œ íŒë‹¨: ì „ì²´ë¹„ìœ¨={total_ratio:.2f}, ìµœê·¼ë¹„ìœ¨={recent_ratio:.2f}")
        
        return recent_ratio >= threshold or total_ratio >= 0.7
    
    def create_monthly_batches(self, df: pd.DataFrame) -> List[pd.DataFrame]:
        """ì›”ë³„ë¡œ ë°°ì¹˜ ìƒì„±"""
        logger.info("ğŸ“¦ ì›”ë³„ ë°°ì¹˜ ìƒì„± ì¤‘...")
        
        # ë‚ ì§œ íŒŒì‹±
        df = self.parse_date_column(df)
        
        # ìœ íš¨í•œ ë‚ ì§œê°€ ìˆëŠ” ë°ì´í„°ë§Œ ì‚¬ìš©
        valid_df = df[df['ë…„ì›”'].notna()].copy()
        
        if len(valid_df) == 0:
            logger.error("âŒ ìœ íš¨í•œ ë‚ ì§œ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return []
        
        # ì›”ë³„ ê·¸ë£¹í™”
        monthly_groups = valid_df.groupby('ë…„ì›”')
        batches = []
        
        # ì›”ë³„ë¡œ ì •ë ¬í•´ì„œ ë°°ì¹˜ ìƒì„±
        sorted_months = sorted(monthly_groups.groups.keys())
        
        for month in sorted_months:
            month_data = monthly_groups.get_group(month)
            batches.append(month_data.copy())
            logger.info(f"   ğŸ“… {month}: {len(month_data)}ê°œ ê±°ë˜")
        
        logger.info(f"ğŸ“¦ ì´ {len(batches)}ê°œ ì›”ë³„ ë°°ì¹˜ ìƒì„±")
        return batches
    
    def preprocess_merchant_name(self, name: str) -> str:
        """ê³ ë„í™”ëœ ê°€ë§¹ì ëª… ì „ì²˜ë¦¬"""
        if pd.isna(name):
            return ""
        
        name = str(name)
        
        # íŠ¹ìˆ˜ë¬¸ì ì •ê·œí™”
        name = re.sub(r'[()[\]{}]', ' ', name)
        name = re.sub(r'[^\wê°€-í£\s]', ' ', name)
        
        # ë¶ˆí•„ìš”í•œ ì ‘ë¯¸ì‚¬ ì œê±°
        suffixes = ['ì ', 'ì§€ì ', 'ë³¸ì ', 'ë§¤ì¥', 'ëŒ€ë¦¬ì ', 'í”„ë¼ì', 'ì„¼í„°', 'ëª°', 'ë§ˆíŠ¸']
        for suffix in suffixes:
            name = re.sub(f'{suffix}$', '', name)
        
        # ê³µë°± ì •ê·œí™”
        name = re.sub(r'\s+', ' ', name)
        
        return name.strip().lower()
    
    def extract_enhanced_features(self, merchants: List[str]) -> np.ndarray:
        """ê³ ë„í™”ëœ íŠ¹ì„± ì¶”ì¶œ"""
        # ê¸°ë³¸ TF-IDF íŠ¹ì„±
        word_features = self.vectorizer.transform(merchants)
        
        # ë¬¸ì ë‹¨ìœ„ TF-IDF íŠ¹ì„±
        char_features = self.char_vectorizer.transform(merchants)
        
        # ìˆ˜ì‘ì—… íŠ¹ì„±
        manual_features = []
        for merchant in merchants:
            features = []
            
            # ê¸¸ì´ íŠ¹ì„±
            features.append(len(merchant))
            features.append(len(merchant.split()))
            
            # ìˆ«ì í¬í•¨ ì—¬ë¶€
            features.append(1 if re.search(r'\d', merchant) else 0)
            
            # ì˜ì–´ í¬í•¨ ì—¬ë¶€
            features.append(1 if re.search(r'[a-zA-Z]', merchant) else 0)
            
            # íŠ¹ìˆ˜ë¬¸ì ê°œìˆ˜
            features.append(len(re.findall(r'[^\wê°€-í£\s]', merchant)))
            
            # í‚¤ì›Œë“œ ë§¤ì¹­ ì ìˆ˜
            keyword_scores = [0] * len(self.CATEGORIES)
            for i, category in enumerate(self.CATEGORIES):
                if category in self.keyword_patterns:
                    score = sum(1 for pattern in self.keyword_patterns[category] 
                              if pattern.search(merchant))
                    keyword_scores[i] = score
            
            features.extend(keyword_scores)
            manual_features.append(features)
        
        manual_features = np.array(manual_features)
        
        # ëª¨ë“  íŠ¹ì„± ê²°í•©
        combined_features = np.hstack([
            word_features.toarray(),
            char_features.toarray(),
            manual_features
        ])
        
        return combined_features
    
    def keyword_based_classification(self, merchant: str) -> Optional[str]:
        """í‚¤ì›Œë“œ ê¸°ë°˜ ë¶„ë¥˜ (ê·œì¹™ ê¸°ë°˜)"""
        merchant_lower = str(merchant).lower()

        category_scores = {}
        
        for category, patterns in self.keyword_patterns.items():
            score = sum(1 for pattern in patterns if pattern.search(merchant_lower))
            if score > 0:
                category_scores[category] = score
        
        if category_scores:
            best_category = max(category_scores.items(), key=lambda x: x[1])
            if best_category[1] >= 1:  # ìµœì†Œ 1ê°œ í‚¤ì›Œë“œ ë§¤ì¹­
                return best_category[0]
        
        return None
    
    def create_ensemble_model(self) -> VotingClassifier:
        """ì•™ìƒë¸” ëª¨ë¸ ìƒì„±"""
        # ê°œë³„ ëª¨ë¸ë“¤
        rf = RandomForestClassifier(
            n_estimators=200,  # 300ì—ì„œ 200ìœ¼ë¡œ ì¤„ì—¬ì„œ ì†ë„ í–¥ìƒ
            max_depth=20,      # 25ì—ì„œ 20ìœ¼ë¡œ ì¤„ì„
            min_samples_split=3,
            min_samples_leaf=2,
            class_weight='balanced',
            random_state=42
        )
        
        lr = LogisticRegression(
            C=1.0,
            class_weight='balanced',
            random_state=42,
            max_iter=1000
        )
        
        svm = SVC(
            C=1.0,
            kernel='rbf',
            class_weight='balanced',
            probability=True,
            random_state=42
        )
        
        # ì†Œí”„íŠ¸ íˆ¬í‘œ ì•™ìƒë¸”
        ensemble = VotingClassifier(
            estimators=[
                ('rf', rf),
                ('lr', lr),
                ('svm', svm)
            ],
            voting='soft'
        )
        
        return ensemble
    
    def _adjust_confidence_threshold(self, accuracy: float):
        """ê°œì„ ëœ ì‹ ë¢°ë„ ì„ê³„ê°’ ë™ì  ì¡°ì •"""
        data_size = len(self.all_training_data)
        
        # ë°ì´í„° í¬ê¸°ì— ë”°ë¥¸ ê¸°ë³¸ ì„ê³„ê°’ ì„¤ì •
        if data_size < 50:
            base_threshold = 0.3
        elif data_size < 100:
            base_threshold = 0.4
        elif data_size < 200:
            base_threshold = 0.5
        else:
            base_threshold = 0.6
        
        # ì •í™•ë„ì— ë”°ë¥¸ ì¡°ì • - ë” ë³´ìˆ˜ì ìœ¼ë¡œ ìˆ˜ì •
        if accuracy > 0.95:
            self.confidence_threshold = min(base_threshold + 0.1, 0.6)  # ìƒí•œì„ 0.6ìœ¼ë¡œ ì œí•œ
        elif accuracy > 0.9:
            self.confidence_threshold = min(base_threshold + 0.05, 0.55)
        elif accuracy > 0.8:
            self.confidence_threshold = min(base_threshold + 0.02, 0.5)
        else:
            self.confidence_threshold = max(base_threshold, self.min_confidence_threshold)

        
        logger.info(f"   ğŸ¯ ì„ê³„ê°’ ì¡°ì •: {self.confidence_threshold:.3f} (ë°ì´í„°: {data_size}ê°œ, ì •í™•ë„: {accuracy:.3f})")
    def retrain_enhanced_model(self, batch_num: int):
        """ê³ ë„í™”ëœ ëª¨ë¸ ì¬í•™ìŠµ (ê¸°ì¡´ ëª¨ë¸ ì—…ë°ì´íŠ¸) - ì™„ì „ ìˆ˜ì •ë¨"""
        logger.info(f"\nğŸ§  ë°°ì¹˜ {batch_num} - ê¸°ì¡´ ëª¨ë¸ ì—…ë°ì´íŠ¸")
        
        # ìœ íš¨í•œ í•™ìŠµ ë°ì´í„° ì„ ë³„
        valid_data = self.all_training_data[
            (self.all_training_data[self.merchant_column].notna()) & 
            (self.all_training_data['ì¹´í…Œê³ ë¦¬'].notna()) &
            (self.all_training_data['ì¹´í…Œê³ ë¦¬'].isin(self.CATEGORIES))
        ].copy()
        
        if len(valid_data) < 10:
            logger.warning("   âš ï¸ í•™ìŠµ ë°ì´í„°ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤.")
            # ë¹ˆ ì„±ëŠ¥ ë”•ì…”ë„ˆë¦¬ë¼ë„ í•„ìˆ˜ í•„ë“œëŠ” í¬í•¨
            empty_performance = {
                'batch': int(batch_num),
                'train_accuracy': 0.0,
                'cv_mean': 0.0,
                'cv_std': 0.0,
                'training_samples': int(len(valid_data)),
                'categories': 0,
                'confidence_threshold': float(self.confidence_threshold),
                'timestamp': datetime.now().isoformat()
            }
            return empty_performance
        
        logger.info(f"   ì „ì²´ ëˆ„ì  í•™ìŠµ ë°ì´í„°: {len(valid_data)}ê°œ")
        
        # íŠ¹ì„± ì¶”ì¶œ ì¤€ë¹„
        X_raw = valid_data[self.merchant_column].apply(self.preprocess_merchant_name)
        y = valid_data['ì¹´í…Œê³ ë¦¬']
        
        # ë²¡í„°ë¼ì´ì € ì´ˆê¸°í™”/ì—…ë°ì´íŠ¸
        if self.vectorizer is None:
            logger.info("   ğŸ”§ ë²¡í„°ë¼ì´ì € ì´ˆê¸°í™”")
            self.vectorizer = TfidfVectorizer(
                max_features=2000,
                ngram_range=(1, 3),
                min_df=1,
                max_df=0.9,
                analyzer='word',
                token_pattern=r'\b\w+\b'
            )
            
            self.char_vectorizer = TfidfVectorizer(
                max_features=1000,
                ngram_range=(2, 4),
                min_df=1,
                max_df=0.9,
                analyzer='char_wb'
            )
        else:
            logger.info("   ğŸ”„ ê¸°ì¡´ ë²¡í„°ë¼ì´ì € ì—…ë°ì´íŠ¸")
        
        # ì „ì²´ ë°ì´í„°ë¡œ ë²¡í„°ë¼ì´ì € ì¬í•™ìŠµ
        self.vectorizer.fit(X_raw)
        self.char_vectorizer.fit(X_raw)
        
        # ê³ ë„í™”ëœ íŠ¹ì„± ì¶”ì¶œ
        X_enhanced = self.extract_enhanced_features(X_raw.tolist())
        X_scaled = self.scaler.fit_transform(X_enhanced)
        
        logger.info(f"   íŠ¹ì„± ìˆ˜: {X_scaled.shape[1]}ê°œ")
        
        # ë¼ë²¨ ì¸ì½”ë”©
        if self.label_encoder is None:
            self.label_encoder = LabelEncoder()
            logger.info("   ğŸ”§ ë¼ë²¨ ì¸ì½”ë” ì´ˆê¸°í™”")
        else:
            logger.info("   ğŸ”„ ê¸°ì¡´ ë¼ë²¨ ì¸ì½”ë” ì—…ë°ì´íŠ¸")
        
        y_encoded = self.label_encoder.fit_transform(y)
        logger.info(f"   ì¹´í…Œê³ ë¦¬ ìˆ˜: {len(self.label_encoder.classes_)}ê°œ")
        
        # ì•™ìƒë¸” ëª¨ë¸ ì¬í•™ìŠµ
        logger.info("   ğŸ¤– ì•™ìƒë¸” ëª¨ë¸ ì¬í•™ìŠµ")
        self.ensemble_classifier = self.create_ensemble_model()
        self.ensemble_classifier.fit(X_scaled, y_encoded)
        
        # êµì°¨ ê²€ì¦ìœ¼ë¡œ ì„±ëŠ¥ í‰ê°€
        try:
            cv_scores = cross_val_score(self.ensemble_classifier, X_scaled, y_encoded, cv=min(5, len(set(y))))
            cv_mean = float(cv_scores.mean())
            cv_std = float(cv_scores.std())
        except Exception as e:
            logger.warning(f"   âš ï¸ êµì°¨ê²€ì¦ ì‹¤íŒ¨: {e}")
            cv_mean, cv_std = 0.0, 0.0
        
        # í›ˆë ¨ ì •í™•ë„
        y_pred = self.ensemble_classifier.predict(X_scaled)
        train_accuracy = float(accuracy_score(y_encoded, y_pred))
        
        # ì‹ ë¢°ë„ ì„ê³„ê°’ ë™ì  ì¡°ì •
        self._adjust_confidence_threshold(train_accuracy)
        
        # ì„±ëŠ¥ ê¸°ë¡ - ëª¨ë“  í•„ë“œë¥¼ ëª…ì‹œì ìœ¼ë¡œ ê¸°ë³¸ íƒ€ì…ìœ¼ë¡œ ìƒì„±
        performance = {
            'batch': int(batch_num),
            'train_accuracy': train_accuracy,
            'cv_mean': cv_mean,
            'cv_std': cv_std,
            'training_samples': int(len(valid_data)),
            'categories': int(len(set(y))),
            'confidence_threshold': float(self.confidence_threshold),
            'timestamp': datetime.now().isoformat()
        }
        
        # performance_historyì— ì¶”ê°€ ì „ì— ê²€ì¦
        logger.info(f"   ğŸ“Š ì„±ëŠ¥ ê¸°ë¡ ì¶”ê°€: batch={performance['batch']}, accuracy={performance['train_accuracy']:.3f}")
        self.performance_history.append(performance)
        
        # ëª¨ë¸ ìë™ ì €ì¥
        self.save_complete_model(batch_num)
        
        logger.info(f"   âœ… ëª¨ë¸ ì—…ë°ì´íŠ¸ ì™„ë£Œ")
        logger.info(f"   í›ˆë ¨ ì •í™•ë„: {train_accuracy:.3f}")
        logger.info(f"   êµì°¨ê²€ì¦ í‰ê· : {cv_mean:.3f} (Â±{cv_std:.3f})")
        logger.info(f"   ì‹ ë¢°ë„ ì„ê³„ê°’: {self.confidence_threshold:.3f}")
        
        return performance
    def classify_batch_with_enhanced_model(self, batch_df: pd.DataFrame, batch_num: int, month: str) -> pd.DataFrame:
        """ê³ ë„í™”ëœ ëª¨ë¸ë¡œ ë°°ì¹˜ ë¶„ë¥˜ (ML â†’ í‚¤ì›Œë“œ â†’ LLM ìˆœì„œë¡œ ë³€ê²½)"""
        logger.info(f"\nğŸ”€ ë°°ì¹˜ {batch_num} ({month}) - í•˜ì´ë¸Œë¦¬ë“œ ë¶„ë¥˜ (ML ìš°ì„ )")
        
        unique_merchants = batch_df[self.merchant_column].unique().tolist()
        logger.info(f"   ê³ ìœ  ê°€ë§¹ì : {len(unique_merchants)}ê°œ")
        
        # 1ë‹¨ê³„: ML ë¶„ë¥˜ ìš°ì„  ìˆ˜í–‰
        ml_results = {}
        remaining_candidates = []
        
        if self.ensemble_classifier is not None:
            try:
                logger.info(f"   ğŸ¤– ML ëª¨ë¸ ìš°ì„  ë¶„ë¥˜ - ì‹ ë¢°ë„ ì„ê³„ê°’: {self.confidence_threshold}")
                
                # ì „ì²˜ë¦¬
                processed_names = [self.preprocess_merchant_name(m) for m in unique_merchants]
                logger.info(f"   ğŸ“ ì „ì²˜ë¦¬ ì™„ë£Œ: {len(processed_names)}ê°œ")
                
                # íŠ¹ì„± ì¶”ì¶œ
                X = self.extract_enhanced_features(processed_names)
                logger.info(f"   ğŸ”§ íŠ¹ì„± ì¶”ì¶œ ì™„ë£Œ: {X.shape}")
                
                # ìŠ¤ì¼€ì¼ë§ ì‹œë„ (ì°¨ì› ë¶ˆì¼ì¹˜ ì²˜ë¦¬ í¬í•¨)
                try:
                    X_scaled = self.scaler.transform(X)
                    logger.info(f"   ğŸ“ ìŠ¤ì¼€ì¼ë§ ì™„ë£Œ: {X_scaled.shape}")
                    
                except ValueError as scale_error:
                    # íŠ¹ì„± ì°¨ì› ë¶ˆì¼ì¹˜ ì˜¤ë¥˜ ê°ì§€
                    if "features" in str(scale_error) and "expecting" in str(scale_error):
                        logger.warning(f"   âš ï¸ íŠ¹ì„± ì°¨ì› ë¶ˆì¼ì¹˜ ê°ì§€!")
                        logger.warning(f"   ì˜¤ë¥˜ ë©”ì‹œì§€: {scale_error}")
                        logger.warning(f"   ğŸ”„ ìƒˆë¡œìš´ íŠ¹ì„±ì— ë§ì¶° ëª¨ë¸ ì¬í•™ìŠµì„ ì‹œì‘í•©ë‹ˆë‹¤...")
                        
                        # í˜„ì¬ ë°°ì¹˜ ë°ì´í„°ë¥¼ ì„ì‹œ ì¹´í…Œê³ ë¦¬ë¡œ ì„¤ì • (í‚¤ì›Œë“œ ë¶„ë¥˜ ë¨¼ì € ì‹œë„)
                        temp_batch_data = batch_df.copy()
                        temp_keyword_results = {}
                        for merchant in unique_merchants:
                            keyword_result = self.keyword_based_classification(merchant)
                            temp_keyword_results[merchant] = keyword_result if keyword_result else 'ê¸°íƒ€'
                        
                        temp_batch_data['ì¹´í…Œê³ ë¦¬'] = temp_batch_data[self.merchant_column].map(temp_keyword_results)
                        temp_batch_data['ì¹´í…Œê³ ë¦¬'] = temp_batch_data['ì¹´í…Œê³ ë¦¬'].fillna('ê¸°íƒ€')
                        
                        # ì„ì‹œë¡œ í•™ìŠµ ë°ì´í„°ì— ì¶”ê°€
                        self.merge_with_existing_data(temp_batch_data)
                        
                        # ëª¨ë¸ ì¬í•™ìŠµ (ìƒˆë¡œìš´ íŠ¹ì„± ì°¨ì›ìœ¼ë¡œ)
                        self.retrain_enhanced_model(batch_num)
                        
                        # ì¬í•™ìŠµ í›„ ë‹¤ì‹œ íŠ¹ì„± ì¶”ì¶œ ë° ìŠ¤ì¼€ì¼ë§ ì‹œë„
                        X = self.extract_enhanced_features(processed_names)
                        X_scaled = self.scaler.transform(X)
                        logger.info(f"   ğŸ”„ ì¬í•™ìŠµ í›„ ìŠ¤ì¼€ì¼ë§ ì™„ë£Œ: {X_scaled.shape}")
                    else:
                        # ë‹¤ë¥¸ ìŠ¤ì¼€ì¼ë§ ì˜¤ë¥˜ëŠ” ì¬ë°œìƒ
                        raise scale_error
                
                # ì˜ˆì¸¡
                probabilities = self.ensemble_classifier.predict_proba(X_scaled)
                logger.info(f"   ğŸ¯ ì˜ˆì¸¡ ì™„ë£Œ: {probabilities.shape}")
                
                # ì‹ ë¢°ë„ ë¶„ì„
                max_confidences = probabilities.max(axis=1)
                high_confidence_count = (max_confidences >= self.confidence_threshold).sum()
                
                logger.info(f"   ğŸ“Š ì‹ ë¢°ë„ ë¶„ì„:")
                logger.info(f"      - í‰ê·  ì‹ ë¢°ë„: {max_confidences.mean():.3f}")
                logger.info(f"      - ìµœëŒ€ ì‹ ë¢°ë„: {max_confidences.max():.3f}")
                logger.info(f"      - ìµœì†Œ ì‹ ë¢°ë„: {max_confidences.min():.3f}")
                logger.info(f"      - ì„ê³„ê°’ ì´ìƒ: {high_confidence_count}ê°œ")
                
                # ì„ì‹œë¡œ ë‚®ì€ ì„ê³„ê°’ìœ¼ë¡œ í…ŒìŠ¤íŠ¸
                for temp_threshold in [0.3, 0.4, 0.5]:
                    temp_count = (max_confidences >= temp_threshold).sum()
                    logger.info(f"      - ì„ê³„ê°’ {temp_threshold} ì´ìƒ: {temp_count}ê°œ")
                
                # ML ë¶„ë¥˜ ê²°ê³¼ ì ìš©
                for i, merchant in enumerate(unique_merchants):
                    probs = probabilities[i]
                    predicted_idx = np.argmax(probs)
                    confidence = probs[predicted_idx]
                    
                    if confidence >= self.confidence_threshold:
                        predicted_category = self.label_encoder.inverse_transform([predicted_idx])[0]
                        ml_results[merchant] = predicted_category
                        logger.debug(f"      âœ… ML ë¶„ë¥˜: {merchant} -> {predicted_category} (ì‹ ë¢°ë„: {confidence:.3f})")
                    else:
                        remaining_candidates.append(merchant)
                        
            except Exception as e:
                logger.error(f"   âš ï¸ ML ë¶„ë¥˜ ì˜¤ë¥˜: {e}")
                logger.error(f"   ìƒì„¸ ì˜¤ë¥˜: {traceback.format_exc()}")
                # ML ì‹¤íŒ¨ ì‹œ ëª¨ë“  í•­ëª©ì„ ë‹¤ìŒ ë‹¨ê³„ë¡œ
                remaining_candidates.extend(unique_merchants)
        else:
            logger.info("   âš ï¸ ML ëª¨ë¸ ì—†ìŒ")
            remaining_candidates.extend(unique_merchants)
        
        logger.info(f"   ML ë¶„ë¥˜: {len(ml_results)}ê°œ")
        logger.info(f"   ë‚¨ì€ í›„ë³´: {len(remaining_candidates)}ê°œ")
        
        # 2ë‹¨ê³„: í‚¤ì›Œë“œ ê¸°ë°˜ ë¶„ë¥˜ (MLì—ì„œ ë¶„ë¥˜ë˜ì§€ ì•Šì€ í•­ëª©ë§Œ)
        keyword_results = {}
        llm_candidates = []
        
        for merchant in remaining_candidates:
            keyword_result = self.keyword_based_classification(merchant)
            if keyword_result:
                keyword_results[merchant] = keyword_result
            else:
                llm_candidates.append(merchant)
        
        logger.info(f"   í‚¤ì›Œë“œ ë¶„ë¥˜: {len(keyword_results)}ê°œ")
        logger.info(f"   LLM í›„ë³´: {len(llm_candidates)}ê°œ")
        
        # 3ë‹¨ê³„: LLM ë¶„ë¥˜ (MLê³¼ í‚¤ì›Œë“œë¡œ ë¶„ë¥˜ë˜ì§€ ì•Šì€ í•­ëª©ë§Œ)
        llm_results = {}
        if llm_candidates and self.ollama_available:
            try:
                llm_results = self.query_llm_for_batch(llm_candidates, batch_num)
            except Exception as e:
                logger.error(f"   âš ï¸ LLM ë¶„ë¥˜ ì˜¤ë¥˜: {e}")
                # LLM ì‹¤íŒ¨ ì‹œ í‚¤ì›Œë“œ ê¸°ë°˜ìœ¼ë¡œ ì²˜ë¦¬
                for merchant in llm_candidates:
                    keyword_result = self.keyword_based_classification(merchant)
                    llm_results[merchant] = keyword_result if keyword_result else 'ê¸°íƒ€'
        else:
            # LLM ì‚¬ìš© ë¶ˆê°€ ì‹œ í‚¤ì›Œë“œ ê¸°ë°˜ìœ¼ë¡œ ì²˜ë¦¬
            for merchant in llm_candidates:
                keyword_result = self.keyword_based_classification(merchant)
                llm_results[merchant] = keyword_result if keyword_result else 'ê¸°íƒ€'
        
        logger.info(f"   LLM ë¶„ë¥˜: {len(llm_candidates)}ê°œ")
        
        # ê²°ê³¼ í†µí•© (ML â†’ í‚¤ì›Œë“œ â†’ LLM ìˆœì„œ)
        all_results = {**ml_results, **keyword_results, **llm_results}
        batch_df['ì¹´í…Œê³ ë¦¬'] = batch_df[self.merchant_column].map(all_results)
        batch_df['ì¹´í…Œê³ ë¦¬'] = batch_df['ì¹´í…Œê³ ë¦¬'].fillna('ê¸°íƒ€')
        
        # ë¶„ë¥˜ ë°©ë²•ë³„ í†µê³„
        logger.info(f"\n   ğŸ“Š ë¶„ë¥˜ ë°©ë²•ë³„ ê²°ê³¼ (ML ìš°ì„ ):")
        logger.info(f"      - ML: {len(ml_results)}ê°œ")
        logger.info(f"      - í‚¤ì›Œë“œ: {len(keyword_results)}ê°œ")
        logger.info(f"      - LLM: {len([k for k in llm_results.keys() if k in llm_candidates])}ê°œ")
        
        # 4ë‹¨ê³„: ëˆ„ì  í•™ìŠµ ë° ì €ì¥
        try:
            logger.info(f"\nğŸ“š ë¶„ë¥˜ ì™„ë£Œ í›„ ëˆ„ì  í•™ìŠµ ì‹œì‘ (ë°°ì¹˜ {batch_num}) - ë°ì´í„° ìˆ˜: {len(batch_df)}")
            
            # ë¨¼ì € í•™ìŠµ ë°ì´í„°ë¥¼ ì—…ë°ì´íŠ¸
            self.update_training_data_with_corrections(batch_df, batch_num)
            
            # ê·¸ ë‹¤ìŒ ëª¨ë¸ì„ ì¬í•™ìŠµ
            self.retrain_enhanced_model(batch_num)
            
            logger.info(f"âœ… ëª¨ë¸ ëˆ„ì  í•™ìŠµ ë° ì €ì¥ ì™„ë£Œ! ì´ ëˆ„ì  ìƒ˜í”Œ ìˆ˜: {len(self.all_training_data)}")

        except Exception as e:
            logger.error(f"âŒ ëˆ„ì  í•™ìŠµ ì‹¤íŒ¨: {e}")
            logger.error(traceback.format_exc())

        return batch_df
    
    def load_excel_data(self, excel_path: str) -> pd.DataFrame:
        """ì—‘ì…€ ë°ì´í„° ë¡œë“œ"""
        logger.info(f"ğŸ“ ì—‘ì…€ íŒŒì¼ ë¡œë“œ: {excel_path}")
        df = pd.read_excel(excel_path)
        logger.info(f"ğŸ“Š ì „ì²´ ë°ì´í„°: {len(df)}ê°œ í–‰")
        
        df_filtered = df[df[self.merchant_column].notna()].copy()
        logger.info(f"ğŸ“Š ìœ íš¨í•œ ë°ì´í„°: {len(df_filtered)}ê°œ í–‰")
        
        return df_filtered
    
    def query_ollama(self, prompt: str, model: str = None, timeout: int = 60) -> str:
        """Ollama í˜¸ì¶œ (ì‹¤ì œ LLM ì²˜ë¦¬)"""
        if model is None:
            model = self.ollama_model
            
        try:
            logger.info(f"ğŸ¤– Ollama í˜¸ì¶œ ì‹œì‘ (ëª¨ë¸: {model})")
            start_time = time.time()
            
            result = subprocess.run(
                ['ollama', 'run', model],
                input=prompt.encode('utf-8'),
                capture_output=True,
                timeout=timeout
            )
            
            elapsed_time = time.time() - start_time
            logger.info(f"ğŸ¤– Ollama í˜¸ì¶œ ì™„ë£Œ ({elapsed_time:.2f}ì´ˆ)")
            
            if result.returncode != 0:
                logger.error(f"Ollama ì˜¤ë¥˜: {result.stderr.decode('utf-8')}")
                raise Exception(f"Ollama ì‹¤í–‰ ì‹¤íŒ¨: {result.stderr.decode('utf-8')}")
            
            return result.stdout.decode('utf-8')
            
        except subprocess.TimeoutExpired:
            logger.error(f"Ollama íƒ€ì„ì•„ì›ƒ ({timeout}ì´ˆ)")
            raise Exception(f"Ollama í˜¸ì¶œ íƒ€ì„ì•„ì›ƒ ({timeout}ì´ˆ)")
        except Exception as e:
            logger.error(f"Ollama í˜¸ì¶œ ì˜¤ë¥˜: {e}")
            raise e
    
    def build_enhanced_llm_prompt(self, merchants: List[str]) -> str:
        """ê³ ë„í™”ëœ LLM í”„ë¡¬í”„íŠ¸"""
        categories_text = ", ".join(self.CATEGORIES)
        
        # ì¹´í…Œê³ ë¦¬ë³„ ì˜ˆì‹œ ì¶”ê°€
        examples = {
            "ì‹ë¹„": "ë§›ì§‘, í•œì‹ë‹¹, ì¤‘êµ­ì§‘, ì¼ì‹ì§‘, ì¹˜í‚¨ì§‘, í”¼ìì§‘",
            "ì¹´í˜/ê°„ì‹": "ìŠ¤íƒ€ë²…ìŠ¤, ì´ë””ì•¼ì»¤í”¼, ë² ì´ì»¤ë¦¬, ë„ë„›, ì•„ì´ìŠ¤í¬ë¦¼",
            "í¸ì˜ì /ë§ˆíŠ¸/ì¡í™”": "GS25, CU, ì´ë§ˆíŠ¸, ë‹¤ì´ì†Œ, ì˜¬ë¦¬ë¸Œì˜",
            "êµí†µ/ëŒ€ì¤‘êµí†µ": "ì§€í•˜ì² , ë²„ìŠ¤, íƒì‹œ, ì¹´ì¹´ì˜¤íƒì‹œ",
            "ìë™ì°¨/ì£¼ìœ ì†Œ": "GSì¹¼í…ìŠ¤, SKì—ë„ˆì§€, ì£¼ìœ ì†Œ, ì„¸ì°¨ì¥",
            "ì‡¼í•‘": "ë°±í™”ì , ì•„ìš¸ë ›, ì˜ë¥˜ë§¤ì¥, ì‹ ë°œê°€ê²Œ",
            "ì˜ë£Œ/ê±´ê°•": "ë³‘ì›, ì•½êµ­, ì¹˜ê³¼, í•œì˜ì›"
        }
        
        prompt = f"""ê°€ë§¹ì ëª…ì„ ì •í™•í•˜ê²Œ ë¶„ë¥˜í•´ì£¼ì„¸ìš”.

ì‚¬ìš© ê°€ëŠ¥í•œ ì¹´í…Œê³ ë¦¬:
{categories_text}

ì¹´í…Œê³ ë¦¬ë³„ ì˜ˆì‹œ:
{chr(10).join([f"- {cat}: {ex}" for cat, ex in examples.items()])}

ë¶„ë¥˜ ê·œì¹™:
1. ê°€ë§¹ì ëª…ì—ì„œ í•µì‹¬ í‚¤ì›Œë“œë¥¼ íŒŒì•…í•˜ì„¸ìš”
2. ê°€ì¥ ì ì ˆí•œ ì¹´í…Œê³ ë¦¬ë¥¼ ì„ íƒí•˜ì„¸ìš”
3. í™•ì‹¤í•˜ì§€ ì•Šìœ¼ë©´ "ê¸°íƒ€"ë¥¼ ì„ íƒí•˜ì„¸ìš”
4. ë°˜ë“œì‹œ JSON í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µí•˜ì„¸ìš”

JSON í˜•ì‹ ì˜ˆì‹œ:
{{"ìŠ¤íƒ€ë²…ìŠ¤ ê°•ë‚¨ì ": "ì¹´í˜/ê°„ì‹", "GS25 ì„œì´ˆì ": "í¸ì˜ì /ë§ˆíŠ¸/ì¡í™”", "ê¹€ë°¥ì²œêµ­": "ì‹ë¹„"}}

ë¶„ë¥˜í•  ê°€ë§¹ì  ëª©ë¡:
"""
        for i, merchant in enumerate(merchants, 1):
            prompt += f"{i}. {merchant}\n"
            
        prompt += "\nJSON ì‘ë‹µ:"
        return prompt
    
    def parse_llm_response(self, response: str) -> Dict[str, str]:
        """LLM ì‘ë‹µ íŒŒì‹± (ê°œì„ ëœ ë²„ì „)"""
        logger.info("ğŸ” LLM ì‘ë‹µ íŒŒì‹± ì¤‘...")
        
        # JSON ë¸”ë¡ ì°¾ê¸° (ì—¬ëŸ¬ íŒ¨í„´ ì‹œë„)
        json_patterns = [
            r'\{[^{}]*(?:\{[^{}]*\}[^{}]*)*\}',  # ì¤‘ì²© ê°€ëŠ¥í•œ JSON
            r'\{.*?\}',  # ê¸°ë³¸ JSON
            r'```json\s*(\{.*?\})\s*```',  # ë§ˆí¬ë‹¤ìš´ JSON ë¸”ë¡
            r'```\s*(\{.*?\})\s*```'  # ì¼ë°˜ ì½”ë“œ ë¸”ë¡
        ]
        
        for pattern in json_patterns:
            matches = re.findall(pattern, response, re.DOTALL)
            for match in matches:
                try:
                    # ë§ˆí¬ë‹¤ìš´ ë¸”ë¡ì—ì„œ ì¶”ì¶œí•œ ê²½ìš°
                    if isinstance(match, tuple):
                        json_str = match[0] if match else match
                    else:
                        json_str = match
                    
                    # JSON íŒŒì‹± ì‹œë„
                    result = json.loads(json_str)
                    
                    # ìœ íš¨í•œ ì¹´í…Œê³ ë¦¬ë§Œ í•„í„°ë§
                    valid_result = {}
                    for key, value in result.items():
                        if value in self.CATEGORIES:
                            valid_result[key] = value
                        else:
                            logger.warning(f"ì˜ëª»ëœ ì¹´í…Œê³ ë¦¬: {value} -> ê¸°íƒ€ë¡œ ë³€ê²½")
                            valid_result[key] = "ê¸°íƒ€"
                    
                    logger.info(f"âœ… JSON íŒŒì‹± ì„±ê³µ: {len(valid_result)}ê°œ í•­ëª©")
                    return valid_result
                    
                except json.JSONDecodeError as e:
                    logger.warning(f"JSON íŒŒì‹± ì‹¤íŒ¨: {e}")
                    continue
        
        # JSON íŒŒì‹± ì‹¤íŒ¨ ì‹œ í…ìŠ¤íŠ¸ íŒŒì‹± ì‹œë„
        logger.warning("JSON íŒŒì‹± ì‹¤íŒ¨, í…ìŠ¤íŠ¸ íŒŒì‹± ì‹œë„...")
        return self._parse_text_response(response)
    
    def _parse_text_response(self, response: str) -> Dict[str, str]:
        """í…ìŠ¤íŠ¸ ì‘ë‹µ íŒŒì‹± (JSON ì‹¤íŒ¨ ì‹œ ë°±ì—…)"""
        result = {}
        lines = response.split('\n')
        
        for line in lines:
            # "ê°€ë§¹ì ëª…: ì¹´í…Œê³ ë¦¬" í˜•ì‹ ì°¾ê¸°
            if ':' in line:
                parts = line.split(':', 1)
                if len(parts) == 2:
                    merchant = parts[0].strip().strip('"\'')
                    category = parts[1].strip().strip('"\'')
                    
                    if category in self.CATEGORIES:
                        result[merchant] = category
            
            # "ê°€ë§¹ì ëª… -> ì¹´í…Œê³ ë¦¬" í˜•ì‹ ì°¾ê¸°
            elif '->' in line:
                parts = line.split('->', 1)
                if len(parts) == 2:
                    merchant = parts[0].strip().strip('"\'')
                    category = parts[1].strip().strip('"\'')
                    
                    if category in self.CATEGORIES:
                        result[merchant] = category
        
        return result
    
    def query_llm_for_batch(self, merchants: List[str], batch_num: int) -> Dict[str, str]:
        """LLM ë¶„ë¥˜ (ë°°ì¹˜ ë‹¨ìœ„)"""
        if not merchants:
            return {}
        
        # ë°°ì¹˜ë¥¼ ë” ì‘ì€ ì²­í¬ë¡œ ë‚˜ëˆ„ê¸° (LLM ì²˜ë¦¬ ì•ˆì •ì„± í–¥ìƒ)
        chunk_size = 10
        all_results = {}
        
        for i in range(0, len(merchants), chunk_size):
            chunk = merchants[i:i + chunk_size]
            logger.info(f"   ì²­í¬ {i//chunk_size + 1}: {len(chunk)}ê°œ ì²˜ë¦¬ ì¤‘...")
            
            prompt = self.build_enhanced_llm_prompt(chunk)
            
            try:
                response = self.query_ollama(prompt)
                results = self.parse_llm_response(response)
                all_results.update(results)
                
                # ì²˜ë¦¬ë˜ì§€ ì•Šì€ í•­ëª©ë“¤ì€ í‚¤ì›Œë“œ ê¸°ë°˜ìœ¼ë¡œ ë¶„ë¥˜
                for merchant in chunk:
                    if merchant not in results:
                        keyword_result = self.keyword_based_classification(merchant)
                        all_results[merchant] = keyword_result if keyword_result else 'ê¸°íƒ€'
                
            except Exception as e:
                logger.error(f"   ì²­í¬ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
                # ì‹¤íŒ¨í•œ ì²­í¬ëŠ” í‚¤ì›Œë“œ ê¸°ë°˜ìœ¼ë¡œ ì²˜ë¦¬
                for merchant in chunk:
                    keyword_result = self.keyword_based_classification(merchant)
                    all_results[merchant] = keyword_result if keyword_result else 'ê¸°íƒ€'
        
        return all_results
    
    def classify_batch_with_llm(self, batch_df: pd.DataFrame, batch_num: int, month: str) -> pd.DataFrame:
        """LLMìœ¼ë¡œ ë°°ì¹˜ ë¶„ë¥˜"""
        logger.info(f"\nğŸ¤– ë°°ì¹˜ {batch_num} ({month}) - LLM ë¶„ë¥˜")
        
        unique_merchants = batch_df[self.merchant_column].unique().tolist()
        logger.info(f"   ê³ ìœ  ê°€ë§¹ì : {len(unique_merchants)}ê°œ")
        
        # Ollama ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸
        if not self.ollama_available:
            logger.warning("   âš ï¸ Ollama ì‚¬ìš© ë¶ˆê°€, í‚¤ì›Œë“œ ê¸°ë°˜ ë¶„ë¥˜ ì‚¬ìš©")
            return self._fallback_keyword_classification(batch_df, unique_merchants)
        
        try:
            llm_results = self.query_llm_for_batch(unique_merchants, batch_num)
        except Exception as e:
            logger.error(f"   âš ï¸ LLM ë¶„ë¥˜ ì‹¤íŒ¨: {e}, í‚¤ì›Œë“œ ê¸°ë°˜ ë¶„ë¥˜ë¡œ ì „í™˜")
            return self._fallback_keyword_classification(batch_df, unique_merchants)
        
        # ê²°ê³¼ ì ìš©
        batch_df['ì¹´í…Œê³ ë¦¬'] = batch_df[self.merchant_column].map(llm_results)
        batch_df['ì¹´í…Œê³ ë¦¬'] = batch_df['ì¹´í…Œê³ ë¦¬'].fillna('ê¸°íƒ€')
        
        return batch_df
    
    def _fallback_keyword_classification(self, batch_df: pd.DataFrame, unique_merchants: List[str]) -> pd.DataFrame:
        """í‚¤ì›Œë“œ ê¸°ë°˜ ë°±ì—… ë¶„ë¥˜"""
        keyword_results = {}
        
        for merchant in unique_merchants:
            keyword_result = self.keyword_based_classification(merchant)
            keyword_results[merchant] = keyword_result if keyword_result else 'ê¸°íƒ€'
        
        batch_df['ì¹´í…Œê³ ë¦¬'] = batch_df[self.merchant_column].map(keyword_results)
        batch_df['ì¹´í…Œê³ ë¦¬'] = batch_df['ì¹´í…Œê³ ë¦¬'].fillna('ê¸°íƒ€')
        
        return batch_df
    
    
    
    def merge_with_existing_data(self, new_batch_data: pd.DataFrame):
        """ìƒˆë¡œìš´ ë°°ì¹˜ ë°ì´í„°ë¥¼ ê¸°ì¡´ í•™ìŠµ ë°ì´í„°ì™€ ë³‘í•© - ìˆ˜ì •ë¨"""
        if len(self.all_training_data) == 0:
            self.all_training_data = new_batch_data.copy()
            logger.info(f"ğŸ“ ì²« ë²ˆì§¸ í•™ìŠµ ë°ì´í„° ì¶”ê°€: {len(new_batch_data)}ê°œ")
        else:
            # ê¸°ì¡´ ë°ì´í„°ì™€ ìƒˆ ë°ì´í„°ë¥¼ ë‹¨ìˆœíˆ concat (ì¤‘ë³µ ì œê±° ì•ˆ í•¨)
            self.all_training_data = pd.concat([self.all_training_data, new_batch_data], ignore_index=True)
            logger.info(f"ğŸ“ ìƒˆë¡œìš´ í•™ìŠµ ë°ì´í„° ì¶”ê°€: {len(new_batch_data)}ê°œ")
        
        logger.info(f"ğŸ“Š ì´ ëˆ„ì  í•™ìŠµ ë°ì´í„°: {len(self.all_training_data)}ê°œ")

    def update_training_data_with_corrections(self, corrected_batch: pd.DataFrame, batch_num: int):
        """ìˆ˜ì •ëœ ë°°ì¹˜ ë°ì´í„°ë¡œ í•™ìŠµ ë°ì´í„° ì—…ë°ì´íŠ¸ - ìˆ˜ì •ë¨"""
        self.current_batch = batch_num
        
        # ê¸°ì¡´ ë°ì´í„°ì™€ ë³‘í•© (í•­ìƒ ì¶”ê°€)
        self.merge_with_existing_data(corrected_batch)
        
        # ìë™ ì €ì¥
        self.save_complete_model(batch_num)
        
        logger.info(f"ğŸ’¾ ë°°ì¹˜ {batch_num} í•™ìŠµ ë°ì´í„° ì—…ë°ì´íŠ¸ ë° ì €ì¥ ì™„ë£Œ")
        
    def manual_correction_step(self, batch_df: pd.DataFrame, batch_num: int, month: str) -> pd.DataFrame:
        """ìˆ˜ì • ë‹¨ê³„ (ì›¹ì—ì„œëŠ” APIë¡œ ëŒ€ì²´)"""
        logger.info(f"\nâœï¸ ë°°ì¹˜ {batch_num} ({month}) ìˆ˜ì • ë‹¨ê³„")
        
        batch_file = f'batch_{batch_num}_{month}_classified.xlsx'
        batch_df.to_excel(batch_file, index=False)
        
        category_counts = batch_df['ì¹´í…Œê³ ë¦¬'].value_counts()
        logger.info(f"   ë¶„ë¥˜ ê²°ê³¼:")
        for category, count in category_counts.head(8).items():
            logger.info(f"   - {category}: {count}ê°œ")
        
        logger.info(f"\nğŸ“ '{batch_file}' íŒŒì¼ì„ ì—´ì–´ì„œ ì§ì ‘ ìˆ˜ì •í•´ì£¼ì„¸ìš”.")
        input("âœ… ìˆ˜ì • ì™„ë£Œ í›„ Enterë¥¼ ëˆŒëŸ¬ì£¼ì„¸ìš”...")
        
        try:
            corrected_df = pd.read_excel(batch_file)
            logger.info(f"   âœ… ìˆ˜ì •ëœ ë°ì´í„° ë¡œë“œ: {len(corrected_df)}ê°œ í–‰")
            
            return corrected_df
        except Exception as e:
            logger.error(f"   âš ï¸ íŒŒì¼ ë¡œë“œ ì˜¤ë¥˜: {e}")
            return batch_df
    
    def process_monthly_incremental_learning(self, excel_path: str):
        """ì›”ë³„ ì ì§„ì  í•™ìŠµ ì‹¤í–‰ (ê¸°ì¡´ ëª¨ë¸ ì—…ë°ì´íŠ¸)"""
        logger.info("ğŸš€ ì§€ì†ì  ëª¨ë¸ ì—…ë°ì´íŠ¸ ì‹œì‘! (ì›”ë³„ ë°°ì¹˜)")
        logger.info(f"ğŸ’¡ ëª¨ë¸ëª…: {self.model_name}")
        logger.info(f"ğŸ’¡ ê¸°ì¡´ í•™ìŠµ ë°ì´í„°: {len(self.all_training_data)}ê°œ")
        logger.info(f"ğŸ’¡ ê¸°ì¡´ ë°°ì¹˜: {len(self.performance_history)}ê°œ")
        
        # ë°ì´í„° ë¡œë“œ ë° ì›”ë³„ ë°°ì¹˜ ìƒì„±
        df = self.load_excel_data(excel_path)
        monthly_batches = self.create_monthly_batches(df)
        
        if not monthly_batches:
            logger.error("âŒ ì²˜ë¦¬í•  ë°°ì¹˜ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        total_batches = len(monthly_batches)
        start_batch = len(self.performance_history) + 1  # ê¸°ì¡´ ë°°ì¹˜ ì´í›„ë¶€í„° ì‹œì‘
        
        for i, batch_df in enumerate(monthly_batches):
            batch_num = start_batch + i
            month = batch_df['ë…„ì›”'].iloc[0] if 'ë…„ì›”' in batch_df.columns else f"ë°°ì¹˜{batch_num}"
            
            logger.info(f"\n{'='*70}")
            logger.info(f"ğŸ“¦ ë°°ì¹˜ {batch_num} ({month}) ì²˜ë¦¬ ì¤‘")
            logger.info(f"   í•´ë‹¹ ì›” ê±°ë˜: {len(batch_df)}ê°œ")
            logger.info(f"   ê¸°ì¡´ ëˆ„ì  í•™ìŠµ ë°ì´í„°: {len(self.all_training_data)}ê°œ")
            logger.info(f"   ì´ ì§„í–‰: {i+1}/{total_batches}")
            logger.info(f"{'='*70}")
            
            # 1ë‹¨ê³„: ë¶„ë¥˜ (ê¸°ì¡´ ëª¨ë¸ì´ ìˆìœ¼ë©´ í•˜ì´ë¸Œë¦¬ë“œ ì‚¬ìš©)
            if self.ensemble_classifier is None:
                # ì²« ë²ˆì§¸ í•™ìŠµì´ê±°ë‚˜ ëª¨ë¸ì´ ì—†ëŠ” ê²½ìš°
                classified_batch = self.classify_batch_with_llm(batch_df, batch_num, month)
            else:
                # ê¸°ì¡´ ëª¨ë¸ì´ ìˆëŠ” ê²½ìš° í•˜ì´ë¸Œë¦¬ë“œ ë¶„ë¥˜
                classified_batch = self.classify_batch_with_enhanced_model(batch_df, batch_num, month)
            
            # 2ë‹¨ê³„: ìˆ˜ë™ ìˆ˜ì •
            corrected_batch = self.manual_correction_step(classified_batch, batch_num, month)
            
            # 3ë‹¨ê³„: ê¸°ì¡´ ëª¨ë¸ ì—…ë°ì´íŠ¸
            performance = self.retrain_enhanced_model(batch_num)
            
            # ê²°ê³¼ ì¶œë ¥
            if performance:
                logger.info(f"\nğŸ“Š ë°°ì¹˜ {batch_num} ({month}) ì™„ë£Œ:")
                logger.info(f"   í˜„ì¬ ë°°ì¹˜: {len(corrected_batch)}ê°œ")
                logger.info(f"   ëˆ„ì  ë°ì´í„°: {performance['training_samples']}ê°œ")
                logger.info(f"   í›ˆë ¨ ì •í™•ë„: {performance['train_accuracy']:.3f}")
                logger.info(f"   êµì°¨ê²€ì¦: {performance['cv_mean']:.3f} (Â±{performance['cv_std']:.3f})")
                logger.info(f"   ì‹ ë¢°ë„ ì„ê³„ê°’: {performance['confidence_threshold']:.3f}")
                
                # ì„±ëŠ¥ ê°œì„  í™•ì¸
                if len(self.performance_history) >= 2:
                    prev_acc = self.performance_history[-2]['train_accuracy']
                    current_acc = performance['train_accuracy']
                    improvement = current_acc - prev_acc
                    if improvement > 0:
                        logger.info(f"   ğŸ“ˆ ì„±ëŠ¥ í–¥ìƒ: +{improvement:.3f}")
                    elif improvement < 0:
                        logger.info(f"   ğŸ“‰ ì„±ëŠ¥ í•˜ë½: {improvement:.3f}")
                    else:
                        logger.info(f"   â¡ï¸ ì„±ëŠ¥ ìœ ì§€")
            
            # ë‹¤ìŒ ë°°ì¹˜ ì§„í–‰ í™•ì¸
            if i + 1 < total_batches:
                continue_process = input(f"\nğŸ”„ ë‹¤ìŒ ë°°ì¹˜ë¥¼ ì§„í–‰í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/n): ")
                if continue_process.lower() != 'y':
                    logger.info("ğŸ›‘ ì²˜ë¦¬ë¥¼ ì¤‘ë‹¨í•©ë‹ˆë‹¤.")
                    break
        
        self.show_enhanced_final_summary()
    
    def show_enhanced_final_summary(self):
        """ê³ ë„í™”ëœ ìµœì¢… ê²°ê³¼ ìš”ì•½"""
        logger.info(f"\nğŸ¯ === ì§€ì†ì  ëª¨ë¸ ì—…ë°ì´íŠ¸ ìµœì¢… ê²°ê³¼ ===")
        logger.info(f"ëª¨ë¸ëª…: {self.model_name}")
        logger.info(f"ì²˜ë¦¬ëœ ë°°ì¹˜: {len(self.performance_history)}ê°œ")
        logger.info(f"ìµœì¢… í•™ìŠµ ë°ì´í„°: {len(self.all_training_data)}ê°œ")
        
        if self.performance_history:
            initial_performance = self.performance_history[0]
            final_performance = self.performance_history[-1]
            
            logger.info(f"\nğŸ“ˆ ì„±ëŠ¥ ë³€í™”:")
            logger.info(f"   ì´ˆê¸° í›ˆë ¨ ì •í™•ë„: {initial_performance['train_accuracy']:.3f}")
            logger.info(f"   ìµœì¢… í›ˆë ¨ ì •í™•ë„: {final_performance['train_accuracy']:.3f}")
            logger.info(f"   ì „ì²´ ê°œì„ : {final_performance['train_accuracy'] - initial_performance['train_accuracy']:+.3f}")
            
            logger.info(f"\nğŸ“Š êµì°¨ê²€ì¦ ì„±ëŠ¥:")
            logger.info(f"   ìµœì¢… CV í‰ê· : {final_performance['cv_mean']:.3f}")
            logger.info(f"   ìµœì¢… CV í‘œì¤€í¸ì°¨: {final_performance['cv_std']:.3f}")
            
            logger.info(f"\nğŸ“Š ìµœê·¼ 5ê°œ ë°°ì¹˜ ì„±ëŠ¥:")
            for perf in self.performance_history[-5:]:
                batch = perf['batch']
                train_acc = perf['train_accuracy']
                cv_mean = perf['cv_mean']
                samples = perf['training_samples']
                logger.info(f"   ë°°ì¹˜ {batch}: í›ˆë ¨={train_acc:.3f}, CV={cv_mean:.3f} (ëˆ„ì  {samples}ê°œ)")
            
            # ìµœì¢… ì¹´í…Œê³ ë¦¬ ë¶„í¬
            if len(self.all_training_data) > 0:
                final_distribution = self.all_training_data['ì¹´í…Œê³ ë¦¬'].value_counts()
                logger.info(f"\nğŸ·ï¸ ìµœì¢… ì¹´í…Œê³ ë¦¬ ë¶„í¬:")
                for category, count in final_distribution.head(12).items():
                    percentage = count / len(self.all_training_data) * 100
                    logger.info(f"   - {category}: {count}ê°œ ({percentage:.1f}%)")
        
        # ëª¨ë¸ ì €ì¥ ìƒíƒœ
        logger.info(f"\nğŸ’¾ ëª¨ë¸ ì €ì¥ ìƒíƒœ:")
        logger.info(f"   ìµœì‹  ëª¨ë¸: {self.model_dir / f'{self.model_name}_latest.pkl'}")
        logger.info(f"   ë°±ì—… ëª¨ë¸: {len(list(self.model_dir.glob(f'{self.model_name}_batch_*.pkl')))}ê°œ")
        logger.info(f"   í•™ìŠµ ë°ì´í„°: {self.data_dir / f'{self.model_name}_training_data.xlsx'}")
    
    def get_model_status(self) -> Dict[str, Any]:
        """í˜„ì¬ ëª¨ë¸ ìƒíƒœ ë°˜í™˜"""
        status = {
            'model_name': self.model_name,
            'model_trained': self.ensemble_classifier is not None,
            'total_training_samples': len(self.all_training_data),
            'total_batches': len(self.performance_history),
            'current_batch': self.current_batch,
            'confidence_threshold': self.confidence_threshold,
            'ollama_available': self.ollama_available,
            'categories_count': len(self.CATEGORIES),
            'keyword_patterns_loaded': len(self.keyword_patterns)
        }
        
        if self.performance_history:
            latest_perf = self.performance_history[-1]
            status.update({
                'latest_accuracy': latest_perf['train_accuracy'],
                'latest_cv_mean': latest_perf['cv_mean'],
                'latest_cv_std': latest_perf['cv_std']
            })
        
        return status
    
    def list_available_models(self) -> List[Dict[str, Any]]:
        """í•˜ë‚˜ì˜ í†µí•© ëª¨ë¸ ì •ë³´ë§Œ ë°˜í™˜"""
        models = []
        
        # í†µí•© ëª¨ë¸ ë©”íƒ€ë°ì´í„°ë§Œ í™•ì¸
        metadata_file = self.model_dir / f'{self.model_name}_metadata.json'
        
        if metadata_file.exists():
            try:
                with open(metadata_file, 'r', encoding='utf-8') as f:
                    metadata = json.load(f)
                models.append(metadata)
            except Exception as e:
                logger.warning(f"ë©”íƒ€ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {metadata_file}, {e}")
        
        return models
    
    def switch_model(self, model_name: str):
        """ëª¨ë¸ ì „í™˜ ë¶ˆê°€ - í•˜ë‚˜ì˜ í†µí•© ëª¨ë¸ë§Œ ì‚¬ìš©"""
        logger.warning("âš ï¸ í•˜ë‚˜ì˜ í†µí•© ëª¨ë¸ë§Œ ì‚¬ìš©ë©ë‹ˆë‹¤. ëª¨ë¸ ì „í™˜ì´ ë¶ˆê°€ëŠ¥í•©ë‹ˆë‹¤.")
        return False


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("ğŸ¯ ì›”ë³„ ë°°ì¹˜ ë¶„í•  ì¹´ë“œ ë¶„ë¥˜ê¸°")
    print("ğŸ’¡ íŠ¹ì§•:")
    print("   - ê¸°ì¡´ í•™ìŠµëœ ëª¨ë¸ ìë™ ë¡œë“œ")
    print("   - ì¹´ë“œì‚¬ìš© ì›”ë³„ ë°°ì¹˜ ë¶„í• ")
    print("   - ML ë¶„ë¥˜ ë””ë²„ê¹… ê°•í™”")
    print("   - í•˜ë‚˜ì˜ ëª¨ë¸ ì§€ì†ì  ì—…ë°ì´íŠ¸")
    print("   - ì‹ ë¢°ë„ ì„ê³„ê°’ ë™ì  ì¡°ì •")
    
    model_name = input("\nğŸ·ï¸ ëª¨ë¸ëª… (ê¸°ë³¸ê°’: default_model): ").strip()
    if not model_name:
        model_name = 'default_model'
    
    excel_file = input("ğŸ“ ì—‘ì…€ íŒŒì¼ ê²½ë¡œ (ê¸°ë³¸ê°’: card.xlsx): ").strip()
    if not excel_file:
        excel_file = 'card.xlsx'
    
    date_column = input("ğŸ“… ë‚ ì§œ ì»¬ëŸ¼ëª… (ê¸°ë³¸ê°’: ì´ìš©ì¼ì): ").strip()
    if not date_column:
        date_column = 'ì´ìš©ì¼ì'
    
    merchant_column = input("ğŸª ê°€ë§¹ì  ì»¬ëŸ¼ëª… (ê¸°ë³¸ê°’: ê°€ë§¹ì ëª…): ").strip()
    if not merchant_column:
        merchant_column = 'ê°€ë§¹ì ëª…'
    
    # ë¶„ë¥˜ê¸° ì´ˆê¸°í™” (ê¸°ì¡´ ëª¨ë¸ ìë™ ë¡œë“œ)
    categorizer = PersistentIncrementalCategorizer(
        merchant_column=merchant_column,
        date_column=date_column,
        model_name=model_name
    )
    
    # ëª¨ë¸ ìƒíƒœ ì¶œë ¥
    status = categorizer.get_model_status()
    print(f"\nğŸ“Š í˜„ì¬ ëª¨ë¸ ìƒíƒœ:")
    print(f"   - ëª¨ë¸ í•™ìŠµë¨: {status['model_trained']}")
    print(f"   - ëˆ„ì  ìƒ˜í”Œ: {status['total_training_samples']}ê°œ")
    print(f"   - ì²˜ë¦¬ëœ ë°°ì¹˜: {status['total_batches']}ê°œ")
    if status['model_trained']:
        print(f"   - í˜„ì¬ ì •í™•ë„: {status.get('latest_accuracy', 0):.3f}")
        print(f"   - ì‹ ë¢°ë„ ì„ê³„ê°’: {status['confidence_threshold']:.3f}")
    
    # ëª¨ë¸ ì§„ë‹¨ ì‹¤í–‰
    if status['model_trained']:
        diagnose = input("\nğŸ” ëª¨ë¸ ì§„ë‹¨ì„ ì‹¤í–‰í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/n): ")
        if diagnose.lower() == 'y':
            categorizer.diagnose_ml_model()
    
    # ì‹ ë¢°ë„ ì„ê³„ê°’ ìˆ˜ë™ ì¡°ì • ì˜µì…˜
    adjust_threshold = input(f"\nğŸ¯ ì‹ ë¢°ë„ ì„ê³„ê°’ì„ ìˆ˜ë™ ì¡°ì •í•˜ì‹œê² ìŠµë‹ˆê¹Œ? í˜„ì¬: {categorizer.confidence_threshold:.3f} (y/n): ")
    if adjust_threshold.lower() == 'y':
        new_threshold = float(input("ìƒˆë¡œìš´ ì„ê³„ê°’ (0.1-0.9): "))
        if 0.1 <= new_threshold <= 0.9:
            categorizer.confidence_threshold = new_threshold
            print(f"âœ… ì„ê³„ê°’ ì¡°ì • ì™„ë£Œ: {new_threshold:.3f}")
        else:
            print("âš ï¸ ì˜ëª»ëœ ê°’ì…ë‹ˆë‹¤. ê¸°ì¡´ ê°’ ìœ ì§€")
    
    # ì›”ë³„ ì ì§„ì  í•™ìŠµ ì‹¤í–‰
    print("\nğŸš€ ì›”ë³„ ì ì§„ì  í•™ìŠµì„ ì‹œì‘í•©ë‹ˆë‹¤...")
    categorizer.process_monthly_incremental_learning(excel_file)


def test_model_functionality():
    """ëª¨ë¸ ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸"""
    print("\nğŸ§ª ëª¨ë¸ ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸:")
    
    # í…ŒìŠ¤íŠ¸ ëª¨ë¸ ìƒì„±
    test_classifier = PersistentIncrementalCategorizer(model_name="test_model")
    
    # ìƒíƒœ í™•ì¸
    status = test_classifier.get_model_status()
    print(f"âœ… í…ŒìŠ¤íŠ¸ ëª¨ë¸ ìƒíƒœ: {status}")
    
    # í‚¤ì›Œë“œ ë¶„ë¥˜ í…ŒìŠ¤íŠ¸
    test_merchants = ["ìŠ¤íƒ€ë²…ìŠ¤ ê°•ë‚¨ì ", "GS25 ì„œì´ˆì ", "ê¹€ë°¥ì²œêµ­", "í˜„ëŒ€ì˜¤ì¼ë±…í¬"]
    print(f"\nğŸ”¤ í‚¤ì›Œë“œ ë¶„ë¥˜ í…ŒìŠ¤íŠ¸:")
    for merchant in test_merchants:
        result = test_classifier.keyword_based_classification(merchant)
        print(f"   {merchant} -> {result if result else 'í‚¤ì›Œë“œ ë§¤ì¹­ ì—†ìŒ'}")


if __name__ == "__main__":
    print("ğŸ¯ ì§€ì†ì  ëª¨ë¸ ì—…ë°ì´íŠ¸ ì¹´ë“œ ë¶„ë¥˜ê¸°")
    print("ğŸ“… ìƒˆë¡œìš´ ê¸°ëŠ¥:")
    print("   âœ… ê¸°ì¡´ í•™ìŠµëœ ëª¨ë¸ ìë™ ë¡œë“œ")
    print("   âœ… ìƒˆ ë°ì´í„°ë¡œ ê¸°ì¡´ ëª¨ë¸ ì—…ë°ì´íŠ¸")
    print("   âœ… ì¤‘ë‹¨ í›„ ì¬ì‹œì‘ ê°€ëŠ¥")
    print("   âœ… ëª¨ë¸ ë°±ì—… ë° ë²„ì „ ê´€ë¦¬")
    print("   âœ… ì—¬ëŸ¬ ëª¨ë¸ ë™ì‹œ ê´€ë¦¬")
    print("   âœ… ì›”ë³„ ë°°ì¹˜ ìë™ ë¶„í• ")
    print("   âœ… ML ë¶„ë¥˜ ë””ë²„ê¹… ê°•í™”")
    
    # ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    test_model_functionality()
    
    # ë©”ì¸ ì‹¤í–‰
    main()